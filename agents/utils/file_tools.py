"""
æ–‡ä»¶æ“ä½œå·¥å…·æ¨¡å—

æä¾›Agentæ‰€éœ€çš„å„ç§æ–‡ä»¶å¤„ç†å·¥å…·å‡½æ•°ã€‚

ä¸»è¦åŠŸèƒ½:
- æ–‡ä»¶è¿‡æ»¤å’Œç­›é€‰
- æ–‡ä»¶å†…å®¹è¯»å–
- ç›®å½•ç»“æ„åˆ†æ
- æ–‡ä»¶è·¯å¾„æå–
"""

import os
import re
import json
from pathlib import Path
from typing import Dict, Any, List, Tuple


def should_filter_file(file_path: str) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿‡æ»¤æ–‡ä»¶
    
    å‚æ•°:
        file_path (str): æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        bool: Trueè¡¨ç¤ºåº”è¯¥è¿‡æ»¤ï¼ŒFalseè¡¨ç¤ºå¯ä»¥åŒ…å«
    """
    # è¿‡æ»¤å¤‡ä»½æ–‡ä»¶
    if '.backup' in file_path or file_path.endswith('.bak'):
        return True
    
    # è¿‡æ»¤è°ƒè¯•è¾“å‡º
    if 'debug_output' in file_path or 'debug_report' in file_path:
        return True
    
    # è¿‡æ»¤éšè—æ–‡ä»¶
    if file_path.startswith('.') or '/.git/' in file_path:
        return True
    
    # è¿‡æ»¤ç¼–è¯‘æ–‡ä»¶
    if '__pycache__' in file_path or file_path.endswith('.pyc'):
        return True
    
    # è¿‡æ»¤æ—¥å¿—æ–‡ä»¶
    if file_path.endswith('.log') or '/logs/' in file_path:
        return True
    
    # è¿‡æ»¤è™šæ‹Ÿç¯å¢ƒ
    if '/venv/' in file_path or '/env/' in file_path:
        return True
    
    return False


def read_files_completely(files_to_read: List[Dict], repo_path: str) -> Dict[str, Any]:
    """
    å®Œæ•´è¯»å–æ–‡ä»¶å†…å®¹
    
    å‚æ•°:
        files_to_read (List[Dict]): éœ€è¦è¯»å–çš„æ–‡ä»¶åˆ—è¡¨
        repo_path (str): ä»“åº“è·¯å¾„
    
    è¿”å›:
        dict: æ–‡ä»¶è¯»å–ç»“æœ
    """
    file_contents = {}
    
    try:
        for file_info in files_to_read:
            file_path = file_info.get("file_path", "")
            
            if not file_path:
                continue
            
            # è¿‡æ»¤ä¸åˆé€‚çš„æ–‡ä»¶
            if should_filter_file(file_path):
                print(f"âš ï¸ è·³è¿‡è¢«è¿‡æ»¤çš„æ–‡ä»¶: {file_path}")
                continue
            
            full_path = os.path.join(repo_path, file_path)
            
            if not os.path.exists(full_path):
                print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                continue
            
            if not os.path.isfile(full_path):
                print(f"âš ï¸ ä¸æ˜¯æ–‡ä»¶: {file_path}")
                continue
            
            try:
                # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œé¿å…è¯»å–è¿‡å¤§çš„æ–‡ä»¶
                file_size = os.path.getsize(full_path)
                if file_size > 500 * 1024:  # 500KBé™åˆ¶
                    print(f"âš ï¸ æ–‡ä»¶è¿‡å¤§ï¼Œè·³è¿‡: {file_path} ({file_size} bytes)")
                    continue
                
                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    file_contents[file_path] = {
                        "content": content,
                        "size": len(content),
                        "lines": len(content.splitlines()),
                        "priority": file_info.get("priority", "medium"),
                        "analysis_focus": file_info.get("analysis_focus", ""),
                        "reason": file_info.get("reason", "")
                    }
                    print(f"âœ… è¯»å–æ–‡ä»¶: {file_path} ({len(content)} å­—ç¬¦)")
                    
            except UnicodeDecodeError:
                print(f"âš ï¸ æ–‡ä»¶ç¼–ç é”™è¯¯ï¼Œè·³è¿‡: {file_path}")
                continue
            except Exception as e:
                print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥: {file_path} - {str(e)}")
                continue
        
        return {
            "file_contents": file_contents,
            "files_read": len(file_contents),
            "total_chars": sum(info["size"] for info in file_contents.values()),
            "success": True
        }
        
    except Exception as e:
        return {
            "file_contents": {},
            "error": f"æ–‡ä»¶è¯»å–è¿‡ç¨‹å¼‚å¸¸: {str(e)}",
            "success": False
        }


def get_basic_file_list(repo_path: str) -> list:
    """
    è·å–åŸºæœ¬çš„æ–‡ä»¶åˆ—è¡¨
    
    å‚æ•°:
        repo_path (str): ä»£ç åº“è·¯å¾„
    
    è¿”å›:
        list: æ–‡ä»¶åˆ—è¡¨
    """
    try:
        files = []
        for root, dirs, filenames in os.walk(repo_path):
            # æ’é™¤éšè—ç›®å½•
            dirs[:] = [d for d in dirs if not d.startswith('.')]
            
            level = root.replace(repo_path, '').count(os.sep)
            indent = '  ' * level
            rel_root = os.path.relpath(root, repo_path)
            
            if rel_root != '.':
                files.append(f"{indent}{os.path.basename(root)}/")
            
            sub_indent = '  ' * (level + 1)
            for filename in filenames:
                if not filename.startswith('.'):
                    files.append(f"{sub_indent}{filename}")
        
        return files
        
    except Exception as e:
        return [f"é”™è¯¯: æ— æ³•åˆ—å‡ºæ–‡ä»¶ - {str(e)}"]


def extract_file_from_error(stdout: str, repo_path: str) -> str:
    """
    ä»é”™è¯¯ä¿¡æ¯ä¸­æå–æ–‡ä»¶è·¯å¾„
    
    å‚æ•°:
        stdout (str): é”™è¯¯è¾“å‡ºä¿¡æ¯
        repo_path (str): ä»“åº“è·¯å¾„
    
    è¿”å›:
        str: æå–åˆ°çš„æ–‡ä»¶è·¯å¾„
    """
    # å¯»æ‰¾å¸¸è§çš„æ–‡ä»¶è·¯å¾„æ¨¡å¼
    patterns = [
        r'File "([^"]+)"',
        r"File '([^']+)'",
        r'in ([a-zA-Z_][a-zA-Z0-9_/\\]*\.py)',
        r'([a-zA-Z_][a-zA-Z0-9_/\\]*\.py)'
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, stdout)
        if matches:
            file_path = matches[0]
            # è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
            if repo_path in file_path:
                return file_path.replace(repo_path, '').lstrip('/')
            return file_path
    
    return ""


def estimate_context_usage(file_contents: Dict) -> Dict:
    """
    ä¼°ç®—ä¸Šä¸‹æ–‡ä½¿ç”¨é‡
    
    å‚æ•°:
        file_contents (Dict): æ–‡ä»¶å†…å®¹å­—å…¸
    
    è¿”å›:
        Dict: ä¼°ç®—ç»“æœ
    """
    total_chars = sum(info["size"] for info in file_contents.values())
    total_lines = sum(info["lines"] for info in file_contents.values())
    
    # ç®€å•ä¼°ç®—tokenæ•°é‡ (å¤§çº¦4ä¸ªå­—ç¬¦ = 1ä¸ªtoken)
    estimated_tokens = total_chars // 4
    
    return {
        "total_files": len(file_contents),
        "total_characters": total_chars,
        "total_lines": total_lines,
        "estimated_tokens": estimated_tokens,
        "context_utilization": min(100, (estimated_tokens / 100000) * 100)  # å‡è®¾æœ€å¤§100k tokens
    }


def create_repo_index(repo_path: str, output_dir: str) -> str:
    """
    åˆ›å»ºä»£ç åº“ç´¢å¼•
    
    å‚æ•°:
        repo_path (str): ä»£ç åº“è·¯å¾„
        output_dir (str): è¾“å‡ºç›®å½•
    
    è¿”å›:
        str: ç´¢å¼•æ–‡ä»¶è·¯å¾„
    """
    try:
        # ä½¿ç”¨ç°æœ‰çš„core_modulesè¿›è¡Œç´¢å¼•
        from core_modules import SimpleStructureAnalyzer, PyDepsAnalyzer
        
        # 1. å…ˆç”Ÿæˆé¡¹ç›®ç»“æ„åˆ†æ
        structure_analyzer = SimpleStructureAnalyzer()
        structure_result = structure_analyzer.analyze_project_structure(repo_path)
        
        # 2. ç”Ÿæˆä¾èµ–å…³ç³»åˆ†æ
        deps_analyzer = PyDepsAnalyzer()
        deps_result = deps_analyzer.analyze_dependencies(repo_path)
        
        # 3. ç»„åˆç»“æœ
        combined_index = {
            "timestamp": get_current_timestamp(),
            "project_name": os.path.basename(repo_path),
            "structure_analysis": structure_result,
            "dependency_analysis": deps_result
        }
        
        # 4. ä¿å­˜ç´¢å¼•æ–‡ä»¶
        index_path = os.path.join(output_dir, "repo_index.json")
        with open(index_path, 'w', encoding='utf-8') as f:
            json.dump(combined_index, f, indent=2, ensure_ascii=False)
        
        print(f"å®Œæˆç»„åˆåˆ†æ: ç»“æ„ + ä¾èµ–å…³ç³»")
        return index_path
        
    except Exception as e:
        print(f"åˆ›å»ºä»£ç åº“ç´¢å¼•å¤±è´¥: {str(e)}")
        return ""


def get_current_timestamp() -> str:
    """è·å–å½“å‰æ—¶é—´æˆ³"""
    from datetime import datetime
    return datetime.now().isoformat() 


def extract_paper_guide_from_markdown(markdown_path: str) -> Dict[str, Any]:
    """
    ä»markdownæ–‡ä»¶ä¸­æå–paper_guideå†…å®¹
    
    å‚æ•°:
        markdown_path (str): markdownæ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        Dict[str, Any]: åŒ…å«æå–ç»“æœçš„å­—å…¸
    """
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(markdown_path):
            raise FileNotFoundError(f"Markdownæ–‡ä»¶ä¸å­˜åœ¨: {markdown_path}")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºmarkdownæ–‡ä»¶
        if not markdown_path.lower().endswith(('.md', '.markdown')):
            raise ValueError(f"ä¸æ˜¯æœ‰æ•ˆçš„Markdownæ–‡ä»¶: {markdown_path}")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(markdown_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # åŸºæœ¬ä¿¡æ¯ç»Ÿè®¡
        lines = content.splitlines()
        word_count = len(content.split())
        char_count = len(content)
        
        # æå–æ ‡é¢˜ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        headers = []
        for line in lines:
            line = line.strip()
            if line.startswith('#'):
                headers.append(line)
        
        return {
            "success": True,
            "paper_guide": content,  # å®Œæ•´çš„markdownå†…å®¹
            "metadata": {
                "file_path": markdown_path,
                "file_name": os.path.basename(markdown_path),
                "char_count": char_count,
                "word_count": word_count,
                "line_count": len(lines),
                "headers": headers[:10]  # æœ€å¤šæ˜¾ç¤ºå‰10ä¸ªæ ‡é¢˜
            }
        }
        
    except FileNotFoundError as e:
        return {
            "success": False,
            "error": f"æ–‡ä»¶æœªæ‰¾åˆ°: {str(e)}",
            "paper_guide": "",
            "metadata": {}
        }
    except UnicodeDecodeError as e:
        return {
            "success": False,
            "error": f"æ–‡ä»¶ç¼–ç é”™è¯¯ï¼Œæ— æ³•è¯»å–: {str(e)}",
            "paper_guide": "",
            "metadata": {}
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"æå–markdownå†…å®¹å¤±è´¥: {str(e)}",
            "paper_guide": "",
            "metadata": {}
        }


def load_paper_guide(guide_source: str) -> str:
    """
    åŠ è½½paper_guideå†…å®¹ï¼ˆæ”¯æŒæ–‡ä»¶è·¯å¾„æˆ–ç›´æ¥å†…å®¹ï¼‰
    
    å‚æ•°:
        guide_source (str): markdownæ–‡ä»¶è·¯å¾„æˆ–ç›´æ¥çš„æ–‡æœ¬å†…å®¹
    
    è¿”å›:
        str: paper_guideå†…å®¹
    """
    # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„
    if guide_source.strip().endswith(('.md', '.markdown')) and os.path.exists(guide_source.strip()):
        result = extract_paper_guide_from_markdown(guide_source.strip())
        if result["success"]:
            print(f"âœ… æˆåŠŸä»markdownæ–‡ä»¶åŠ è½½paper_guide: {result['metadata']['file_name']}")
            print(f"   æ–‡ä»¶ä¿¡æ¯: {result['metadata']['char_count']} å­—ç¬¦, {result['metadata']['line_count']} è¡Œ")
            return result["paper_guide"]
        else:
            print(f"âŒ åŠ è½½markdownæ–‡ä»¶å¤±è´¥: {result['error']}")
            return ""
    
    # å¦åˆ™å½“ä½œç›´æ¥å†…å®¹è¿”å›
    return guide_source


def load_additional_guides(guide_paths: List[str]) -> Dict[str, Any]:
    """
    åŠ è½½å¤šä¸ªè¡¥å……ä¿¡æ¯æ–‡æ¡£å¹¶æ‹¼æ¥æˆç»Ÿä¸€å†…å®¹
    
    ç”¨äºè®ºæ–‡å¤ç°ç³»ç»Ÿä¸­çš„è¡¥å……ä¿¡æ¯å¤„ç†ï¼Œå°†å¤šä¸ªmarkdownæ–‡æ¡£æ‹¼æ¥
    æˆä¸€ä¸ªå®Œæ•´çš„è¡¥å……ä¿¡æ¯å­—ç¬¦ä¸²ï¼Œæ–¹ä¾¿ä¼ é€’ç»™Analyzerå’ŒCoderã€‚
    
    å‚æ•°:
        guide_paths (List[str]): markdownæ–‡æ¡£è·¯å¾„åˆ—è¡¨
    
    è¿”å›:
        Dict[str, Any]: åŒ…å«ä»¥ä¸‹é”®å€¼çš„å­—å…¸:
            - success (bool): æ˜¯å¦æˆåŠŸå¤„ç†æ‰€æœ‰æ–‡æ¡£
            - additional_content (str): æ‹¼æ¥åçš„è¡¥å……ä¿¡æ¯å†…å®¹
            - processed_files (List[str]): æˆåŠŸå¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
            - failed_files (List[Dict]): å¤„ç†å¤±è´¥çš„æ–‡ä»¶åŠé”™è¯¯ä¿¡æ¯
            - metadata (Dict): æ•´ä½“ç»Ÿè®¡ä¿¡æ¯
    """
    if not guide_paths:
        return {
            "success": True,
            "additional_content": "",
            "processed_files": [],
            "failed_files": [],
            "metadata": {
                "total_files": 0,
                "total_char_count": 0,
                "total_line_count": 0
            }
        }
    
    processed_files = []
    failed_files = []
    content_parts = []
    total_char_count = 0
    total_line_count = 0
    
    # æ·»åŠ è¡¥å……ä¿¡æ¯å¼€å§‹æ ‡è®°
    content_parts.append("=== è¡¥å……ä¿¡æ¯æ±‡æ€» ===\n")
    
    # é€ä¸ªå¤„ç†æ¯ä¸ªæ–‡æ¡£
    for i, guide_path in enumerate(guide_paths, 1):
        try:
            # éªŒè¯è·¯å¾„æ ¼å¼å’Œæ–‡ä»¶å­˜åœ¨æ€§
            guide_path = guide_path.strip()
            if not guide_path.endswith(('.md', '.markdown')):
                failed_files.append({
                    "file_path": guide_path,
                    "error": "æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒï¼Œä»…æ”¯æŒ.mdå’Œ.markdownæ–‡ä»¶"
                })
                continue
            
            if not os.path.exists(guide_path):
                failed_files.append({
                    "file_path": guide_path,
                    "error": "æ–‡ä»¶ä¸å­˜åœ¨"
                })
                continue
            
            # æå–æ–‡æ¡£å†…å®¹
            result = extract_paper_guide_from_markdown(guide_path)
            if result["success"]:
                # æˆåŠŸæå–å†…å®¹
                file_name = result['metadata']['file_name']
                content = result['paper_guide']
                char_count = result['metadata']['char_count']
                line_count = result['metadata']['line_count']
                
                # æ·»åŠ æ–‡æ¡£åˆ†éš”æ ‡è®°å’Œå†…å®¹
                content_parts.append(f"\n## è¡¥å……æ–‡æ¡£{i}: {file_name}\n")
                content_parts.append(f"<!-- æ¥æº: {guide_path} -->\n")
                content_parts.append(content)
                content_parts.append("\n")
                
                # è®°å½•æˆåŠŸå¤„ç†çš„æ–‡ä»¶
                processed_files.append({
                    "file_path": guide_path,
                    "file_name": file_name,
                    "char_count": char_count,
                    "line_count": line_count
                })
                
                # ç´¯è®¡ç»Ÿè®¡
                total_char_count += char_count
                total_line_count += line_count
                
                print(f"âœ… æˆåŠŸåŠ è½½è¡¥å……æ–‡æ¡£{i}: {file_name} ({char_count}å­—ç¬¦, {line_count}è¡Œ)")
                
            else:
                # æå–å¤±è´¥
                failed_files.append({
                    "file_path": guide_path,
                    "error": result['error']
                })
                print(f"âŒ åŠ è½½è¡¥å……æ–‡æ¡£{i}å¤±è´¥: {guide_path} - {result['error']}")
                
        except Exception as e:
            # å¤„ç†å¼‚å¸¸
            failed_files.append({
                "file_path": guide_path,
                "error": f"å¤„ç†å¼‚å¸¸: {str(e)}"
            })
            print(f"âŒ å¤„ç†è¡¥å……æ–‡æ¡£{i}æ—¶å‡ºç°å¼‚å¸¸: {guide_path} - {str(e)}")
    
    # æ·»åŠ è¡¥å……ä¿¡æ¯ç»“æŸæ ‡è®°
    content_parts.append("\n=== è¡¥å……ä¿¡æ¯ç»“æŸ ===\n")
    
    # æ‹¼æ¥æ‰€æœ‰å†…å®¹
    additional_content = "\n".join(content_parts)
    
    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
    final_char_count = len(additional_content)
    final_line_count = len(additional_content.splitlines())
    
    # åˆ¤æ–­æ•´ä½“æ˜¯å¦æˆåŠŸ
    success = len(processed_files) > 0  # è‡³å°‘æœ‰ä¸€ä¸ªæ–‡ä»¶æˆåŠŸå¤„ç†
    
    # è¾“å‡ºå¤„ç†ç»“æœæ‘˜è¦
    if success:
        print(f"ğŸ“‹ è¡¥å……ä¿¡æ¯å¤„ç†å®Œæˆ:")
        print(f"   æˆåŠŸå¤„ç†: {len(processed_files)}/{len(guide_paths)} ä¸ªæ–‡ä»¶")
        print(f"   æ€»å†…å®¹: {final_char_count} å­—ç¬¦, {final_line_count} è¡Œ")
        if failed_files:
            print(f"   å¤„ç†å¤±è´¥: {len(failed_files)} ä¸ªæ–‡ä»¶")
    else:
        print(f"âŒ è¡¥å……ä¿¡æ¯å¤„ç†å¤±è´¥: æ‰€æœ‰ {len(guide_paths)} ä¸ªæ–‡ä»¶éƒ½æ— æ³•å¤„ç†")
    
    return {
        "success": success,
        "additional_content": additional_content if success else "",
        "processed_files": processed_files,
        "failed_files": failed_files,
        "metadata": {
            "total_files": len(guide_paths),
            "processed_count": len(processed_files),
            "failed_count": len(failed_files),
            "total_char_count": total_char_count,
            "total_line_count": total_line_count,
            "final_char_count": final_char_count,
            "final_line_count": final_line_count
        }
    }


if __name__ == "__main__":
    #æµ‹è¯•è·å¾—æ–‡ä»¶åˆ—è¡¨
    repo_path = "/Users/zhaoyu/Desktop/test_input/webpage"
    file_list = get_basic_file_list(repo_path)
    print(file_list)

    # æµ‹è¯•markdownæå–åŠŸèƒ½
    test_md_path = "test_papers/paper_test_1_reproduction_guide.md"
    if os.path.exists(test_md_path):
        result = extract_paper_guide_from_markdown(test_md_path)
        if result["success"]:
            print("âœ… Markdownæå–æµ‹è¯•æˆåŠŸ")
            print(f"å†…å®¹é•¿åº¦: {len(result['paper_guide'])} å­—ç¬¦")
            print(f"æ ‡é¢˜æ•°é‡: {len(result['metadata']['headers'])}")
            print(result['paper_guide'])
        else:
            print(f"âŒ Markdownæå–æµ‹è¯•å¤±è´¥: {result['error']}")
    else:
        print("âš ï¸ æµ‹è¯•markdownæ–‡ä»¶ä¸å­˜åœ¨")