"""
å¢å¼ºç‰ˆåˆ†æAgent

å®ç°å¤šé˜¶æ®µåˆ†ææµç¨‹ï¼š
1. æ–‡ä»¶è¯†åˆ«é˜¶æ®µï¼šåŸºäºstdout + RepoIndexè¯†åˆ«éœ€è¦è¯»å–çš„æ–‡ä»¶
2. æ–‡ä»¶è¯»å–é˜¶æ®µï¼šå®Œæ•´è¯»å–æ–‡ä»¶å†…å®¹
3. æ·±åº¦åˆ†æé˜¶æ®µï¼šåŸºäºå®Œæ•´ä¿¡æ¯è¿›è¡Œåˆ†æ
4. å¤šä»»åŠ¡ç”Ÿæˆï¼šç”Ÿæˆæœ‰ä¾èµ–å…³ç³»çš„ä¿®å¤ä»»åŠ¡åˆ—è¡¨

ä¸»è¦åŠŸèƒ½:
- analyze_error(): å¤šé˜¶æ®µé”™è¯¯åˆ†æå’Œä»»åŠ¡ç”Ÿæˆ
- _identify_relevant_files(): è¯†åˆ«ç›¸å…³æ–‡ä»¶
- _read_files_completely(): å®Œæ•´è¯»å–æ–‡ä»¶å†…å®¹
- _deep_analysis_with_content(): åŸºäºå®Œæ•´å†…å®¹çš„æ·±åº¦åˆ†æ
- _generate_task_sequence(): ç”Ÿæˆä»»åŠ¡åºåˆ—
"""

import sys

import json
import os
import re
from pathlib import Path
from typing import Dict, Any, List, Tuple
from .base_agent import BaseAgent
from .utils import (
    file_tools, json_tools, analysis_tools,
    read_files_completely, parse_json_response, 
    validate_and_enhance_tasks, 
    generate_execution_plan,
    generate_fallback_result, extract_file_from_error,
    get_current_timestamp, get_basic_file_list,
    estimate_context_usage, create_repo_index
)


class AnalyzerAgent(BaseAgent):
    """
    å¢å¼ºç‰ˆåˆ†æAgent
    
    é€šè¿‡å¤šé˜¶æ®µåˆ†ææµç¨‹ï¼Œä»é”™è¯¯è¾“å‡ºåˆ°å®Œæ•´æ–‡ä»¶å†…å®¹çš„æ·±åº¦ç†è§£ï¼Œ
    ç”Ÿæˆç³»ç»Ÿæ€§çš„å¤šä»»åŠ¡ä¿®å¤æ–¹æ¡ˆã€‚
    """
    
    def __init__(self, **kwargs):
        """
        åˆå§‹åŒ–åˆ†æAgent
        
        å‚æ•°:
            **kwargs: é…ç½®å‚æ•°
        """
        super().__init__(**kwargs)
        self.max_files_per_analysis = 10  # æ¯æ¬¡åˆ†æçš„æœ€å¤§æ–‡ä»¶æ•°
        self.max_tokens_estimate = 100000  # é¢„ä¼°çš„æœ€å¤§tokenæ•°

    def analyze_error(self, stdout: str, repo_path: str, indexed_repo_data: Dict[str, Any] = None, expected_behavior: str = None) -> Dict[str, Any]:
        """
        å¤šé˜¶æ®µé”™è¯¯åˆ†æå’Œä»»åŠ¡ç”Ÿæˆ
        
        å‚æ•°:
            stdout (str): ç¨‹åºè¾“å‡ºï¼ˆåŒ…å«é”™è¯¯ä¿¡æ¯ï¼‰
            repo_path (str): ä»£ç åº“è·¯å¾„
            indexed_repo_data (dict, optional): ä»£ç åº“ç´¢å¼•æ•°æ®
            expected_behavior (str, optional): æœŸæœ›çš„ç¨‹åºè¡Œä¸ºæè¿°
        
        è¿”å›:
            dict: å¤šä»»åŠ¡åˆ†æç»“æœ
            {
                "analysis_stages": {
                    "file_identification": {...},
                    "file_reading": {...},
                    "deep_analysis": {...}
                },
                "tasks": [
                    {
                        "task_id": str,
                        "priority": int,
                        "fixing_type": str,
                        "which_file_to_fix": str,
                        "fixing_plan_in_detail": str,
                        "raw_code": str,
                        "dependencies": [str],
                        "estimated_impact": str
                    }
                ],
                "execution_plan": {
                    "total_tasks": int,
                    "execution_order": [str],
                    "risk_assessment": str
                }
            }
        """
        try:
            # å¦‚æœæ²¡æœ‰æä¾›ç´¢å¼•æ•°æ®ï¼Œå°è¯•ç”Ÿæˆ
            if not indexed_repo_data:
                indexed_repo_data = self._get_or_create_repo_index(repo_path)
            
            print("ğŸ” å¼€å§‹å¤šé˜¶æ®µé”™è¯¯åˆ†æ...")
            
            # é˜¶æ®µ1: è¯†åˆ«ç›¸å…³æ–‡ä»¶
            print("ğŸ“‹ é˜¶æ®µ1: è¯†åˆ«éœ€è¦è¯»å–çš„æ–‡ä»¶")
            file_identification_result = self._identify_relevant_files(stdout, indexed_repo_data, expected_behavior)
            
            if not file_identification_result.get("files_to_read"):
                return generate_fallback_result("æ— æ³•è¯†åˆ«ç›¸å…³æ–‡ä»¶")
            
            # é˜¶æ®µ2: å®Œæ•´è¯»å–æ–‡ä»¶å†…å®¹
            print("ğŸ“– é˜¶æ®µ2: è¯»å–å®Œæ•´æ–‡ä»¶å†…å®¹")
            file_reading_result = read_files_completely(
                file_identification_result["files_to_read"], 
                repo_path
            )
            
            if not file_reading_result.get("file_contents"):
                return generate_fallback_result("æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹")
            
            # é˜¶æ®µ3: åŸºäºå®Œæ•´å†…å®¹çš„æ·±åº¦åˆ†æ
            print("ğŸ§  é˜¶æ®µ3: æ·±åº¦åˆ†æå’Œå¤šä»»åŠ¡ç”Ÿæˆ")
            deep_analysis_result = self._deep_analysis_with_content(
                stdout, 
                indexed_repo_data, 
                file_reading_result["file_contents"],
                expected_behavior
            )
            
            # æ„å»ºæœ€ç»ˆç»“æœ
            result = {
                "analysis_stages": {
                    "file_identification": file_identification_result,
                    "file_reading": file_reading_result,
                    "deep_analysis": deep_analysis_result
                },
                "tasks": deep_analysis_result.get("tasks", []),
                "execution_plan": generate_execution_plan(deep_analysis_result.get("tasks", []))
            }
            
            print(f"âœ… åˆ†æå®Œæˆ: ç”Ÿæˆäº† {len(result['tasks'])} ä¸ªä¿®å¤ä»»åŠ¡")
            return result
            
        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹å‡ºç°å¼‚å¸¸: {str(e)}")
            return generate_fallback_result(f"åˆ†æå¼‚å¸¸: {str(e)}")

    def _identify_relevant_files(self, stdout: str, indexed_repo_data: Dict[str, Any], expected_behavior: str = None) -> Dict[str, Any]:
        """
        é˜¶æ®µ1: è¯†åˆ«éœ€è¦è¯»å–çš„ç›¸å…³æ–‡ä»¶
        
        å‚æ•°:
            stdout (str): ç¨‹åºé”™è¯¯è¾“å‡º
            indexed_repo_data (dict): ä»£ç åº“ç´¢å¼•æ•°æ®
            expected_behavior (str, optional): æœŸæœ›çš„ç¨‹åºè¡Œä¸ºæè¿°
        
        è¿”å›:
            dict: æ–‡ä»¶è¯†åˆ«ç»“æœ
        """
        try:
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç é”™è¯¯åˆ†æä¸“å®¶ã€‚è¯·åŸºäºç¨‹åºé”™è¯¯è¾“å‡ºå’Œé¡¹ç›®ç´¢å¼•ï¼Œè¯†åˆ«éœ€è¦è¯»å–çš„ç›¸å…³æ–‡ä»¶ã€‚

                                è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š
                                {
                                    "files_to_read": [
                                        {
                                            "file_path": "ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„æ–‡ä»¶è·¯å¾„",
                                            "reason": "éœ€è¦è¯»å–æ­¤æ–‡ä»¶çš„åŸå› ",
                                            "priority": "high/medium/low",
                                            "analysis_focus": "åœ¨æ­¤æ–‡ä»¶ä¸­éœ€è¦é‡ç‚¹å…³æ³¨çš„å†…å®¹"
                                        }
                                    ],
                                    "analysis_reasoning": "æ–‡ä»¶é€‰æ‹©çš„æ•´ä½“reasoning"
                                }

                                æ–‡ä»¶è¯†åˆ«ç­–ç•¥ï¼š
                                1. ç›´æ¥é”™è¯¯æ–‡ä»¶ï¼šä»é”™è¯¯å †æ ˆä¸­æå–çš„ç›´æ¥ç›¸å…³æ–‡ä»¶
                                2. ä¾èµ–æ–‡ä»¶ï¼šæ ¹æ®è°ƒç”¨å›¾åˆ†æçš„ä¸Šä¸‹æ¸¸æ–‡ä»¶
                                3. é…ç½®æ–‡ä»¶ï¼šå¯èƒ½å½±å“è¡Œä¸ºçš„é…ç½®æ–‡ä»¶
                                4. æµ‹è¯•æ–‡ä»¶ï¼šç›¸å…³çš„æµ‹è¯•æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                                5. æœŸæœ›è¡Œä¸ºç›¸å…³ï¼šä¸å®ç°æœŸæœ›ç¨‹åºè¡Œä¸ºç›´æ¥ç›¸å…³çš„æ–‡ä»¶
                                6. é™åˆ¶æ•°é‡ï¼šæœ€å¤šé€‰æ‹©10ä¸ªæœ€ç›¸å…³çš„æ–‡ä»¶

                                ä¼˜å…ˆçº§è¯´æ˜ï¼š
                                - high: é”™è¯¯ç›´æ¥å‘ç”Ÿçš„æ–‡ä»¶ï¼Œå¿…é¡»ä¿®å¤
                                - medium: è°ƒç”¨é“¾ç›¸å…³æ–‡ä»¶ï¼Œå¯èƒ½éœ€è¦ä¿®æ”¹
                                - low: é…ç½®æˆ–æµ‹è¯•æ–‡ä»¶ï¼Œå¯èƒ½éœ€è¦æ›´æ–°

                                ğŸš« ä¸¥æ ¼ç¦æ­¢é€‰æ‹©ä»¥ä¸‹ç±»å‹çš„æ–‡ä»¶ï¼š
                                - å¤‡ä»½æ–‡ä»¶ï¼š*.backup, *.backup_*, *.bak
                                - è°ƒè¯•è¾“å‡ºï¼šdebug_output/*, debug_report*, modification_history*
                                - éšè—æ–‡ä»¶ï¼š.*, .*/*
                                - ç¼–è¯‘æ–‡ä»¶ï¼š__pycache__/*, *.pyc, *.pyo
                                - ç‰ˆæœ¬æ§åˆ¶ï¼š.git/*, .svn/*
                                - IDEæ–‡ä»¶ï¼š.vscode/*, .idea/*
                                - è™šæ‹Ÿç¯å¢ƒï¼švenv/*, env/*, virtualenv/*
                                - ä¾èµ–ç›®å½•ï¼šnode_modules/*
                                - æ—¥å¿—æ–‡ä»¶ï¼š*.log, logs/*
                                - ä¸´æ—¶æ–‡ä»¶ï¼š*.tmp, *.temp
                                - ç³»ç»Ÿæ–‡ä»¶ï¼š.DS_Store, Thumbs.db

                                âœ… åªé€‰æ‹©çœŸæ­£çš„æºä»£ç æ–‡ä»¶å’Œé…ç½®æ–‡ä»¶ï¼š
                                - Pythonæºç ï¼š*.py
                                - é…ç½®æ–‡ä»¶ï¼š*.json, *.yaml, *.yml, *.toml, *.ini, *.cfg
                                - æ–‡æ¡£æ–‡ä»¶ï¼š*.md, *.txt
                                - Webæ–‡ä»¶ï¼š*.html, *.css, *.js
                                - å…¶ä»–æºç ï¼š*.java, *.cpp, *.c, *.hç­‰"""

                                            # æ„å»ºç”¨æˆ·æç¤ºè¯
            user_prompt = f"""## ç¨‹åºé”™è¯¯è¾“å‡º
                                ```
                                {stdout}
                                ```

                                ## é¡¹ç›®ç´¢å¼•ä¿¡æ¯
                                ```json
                                {json.dumps({
                                    "project_name": indexed_repo_data.get("project_name", ""),
                                    "directory_structure": indexed_repo_data.get("directory_structure", [])[:20],
                                    "files": {k: v for i, (k, v) in enumerate(indexed_repo_data.get("files", {}).items()) if i < 8},
                                    "function_dependencies": {
                                        "has_analysis": indexed_repo_data.get("analysis_info", {}).get("has_dependency_analysis", False),
                                        "call_graph_summary": self._summarize_call_graph(indexed_repo_data.get("function_dependencies", {}))
                                    }
                                }, indent=2, ensure_ascii=False)}
                                ```

                                è¯·åˆ†æé”™è¯¯è¾“å‡ºï¼Œè¯†åˆ«éœ€è¦è¯»å–çš„å…³é”®æ–‡ä»¶ã€‚é‡ç‚¹è€ƒè™‘ï¼š
                                1. é”™è¯¯å †æ ˆä¸­ç›´æ¥æåˆ°çš„æ–‡ä»¶
                                2. å¯èƒ½åŒ…å«é”™è¯¯æ ¹å› çš„æ–‡ä»¶
                                3. éœ€è¦ä¿®æ”¹çš„ç›¸å…³ä¾èµ–æ–‡ä»¶
                                4. å¯èƒ½éœ€è¦æ›´æ–°çš„é…ç½®æˆ–æµ‹è¯•æ–‡ä»¶"""
            
            # æ·»åŠ æœŸæœ›è¡Œä¸ºæŒ‡å¯¼
            expected_behavior_section = ""
            if expected_behavior:
                expected_behavior_section = f"""
                                
                                ## æœŸæœ›çš„ç¨‹åºè¡Œä¸º
                                ```
                                {expected_behavior}
                                ```
                                
                                åœ¨è¯†åˆ«ç›¸å…³æ–‡ä»¶æ—¶ï¼Œè¯·ç‰¹åˆ«å…³æ³¨ï¼š
                                5. ä¸å®ç°æœŸæœ›è¡Œä¸ºç›¸å…³çš„æ ¸å¿ƒæ–‡ä»¶
                                6. å¯èƒ½é˜»æ­¢ç¨‹åºè¾¾åˆ°æœŸæœ›è¡Œä¸ºçš„æ–‡ä»¶
                                7. éœ€è¦ä¿®æ”¹ä»¥ç¡®ä¿ç¬¦åˆæœŸæœ›è¡Œä¸ºçš„é…ç½®æ–‡ä»¶
                                
                                è¯·ç¡®ä¿é€‰æ‹©çš„æ–‡ä»¶æœ‰åŠ©äºè®©ç¨‹åºæœ€ç»ˆå®ç°æœŸæœ›çš„è¡Œä¸ºã€‚"""
            
            user_prompt += expected_behavior_section

            print('\n\n')

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            response = self.call_llm(messages, max_tokens=16384, temperature=0.2)
            
            # è§£æJSONå“åº”
            result = parse_json_response(response)
            
            # éªŒè¯å’Œæ¸…ç†ç»“æœ
            if "files_to_read" in result:
                # é™åˆ¶æ–‡ä»¶æ•°é‡
                result["files_to_read"] = result["files_to_read"][:self.max_files_per_analysis]
                
                # éªŒè¯æ–‡ä»¶è·¯å¾„æ ¼å¼
                for file_info in result["files_to_read"]:
                    if "priority" not in file_info:
                        file_info["priority"] = "medium"
                    if "analysis_focus" not in file_info:
                        file_info["analysis_focus"] = "æ•´ä½“ä»£ç é€»è¾‘"
            
            return result
            
        except Exception as e:
            return {
                "files_to_read": [],
                "analysis_reasoning": f"æ–‡ä»¶è¯†åˆ«å¤±è´¥: {str(e)}",
                "error": str(e)
            }

    def _deep_analysis_with_content(self, stdout: str, indexed_repo_data: Dict[str, Any], 
                                file_contents: Dict[str, Any], expected_behavior: str = None) -> Dict[str, Any]:
        """
        é˜¶æ®µ3: åŸºäºå®Œæ•´æ–‡ä»¶å†…å®¹çš„æ·±åº¦åˆ†æ
        
        å‚æ•°:
            stdout (str): ç¨‹åºé”™è¯¯è¾“å‡º
            indexed_repo_data (dict): ä»£ç åº“ç´¢å¼•æ•°æ®
            file_contents (dict): å®Œæ•´æ–‡ä»¶å†…å®¹
            expected_behavior (str, optional): æœŸæœ›çš„ç¨‹åºè¡Œä¸ºæè¿°
        
        è¿”å›:
            dict: æ·±åº¦åˆ†æç»“æœï¼ŒåŒ…å«å¤šä¸ªä»»åŠ¡
        """
        try:
            system_prompt = """ä½ æ˜¯ä¸“ä¸šçš„ä»£ç é”™è¯¯ä¿®å¤ä¸“å®¶ã€‚ä½ çš„é¦–è¦ä¸”å”¯ä¸€ç›®æ ‡æ˜¯ï¼šåˆ†æé”™è¯¯åŸå› å¹¶æŒ‰ç…§è¾“å‡ºæ ¼å¼è¦æ±‚ç»™å‡ºè¯¦ç»†çš„ä¿®å¤æ–¹æ¡ˆï¼Œè®©ç¨‹åºèƒ½å¤ŸæˆåŠŸè¿è¡Œã€‚
                                ä½ å°†ä¼šæ ¹æ®ç”¨æˆ·æä¾›çš„ä¿¡æ¯æ¥ç»™å‡ºä½ çš„è§£å†³æ–¹æ¡ˆï¼Œå…·ä½“å½¢å¼æ˜¯ç»™å‡ºä¸€ä¸ªtaskåˆ—è¡¨

                                è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š
                                {
                                    "root_cause_analysis": "é”™è¯¯çš„æ ¹æœ¬åŸå› åˆ†æ",
                                    "impact_analysis": "é”™è¯¯å½±å“èŒƒå›´å’Œä¼ æ’­è·¯å¾„åˆ†æ",
                                    "tasks": [
                                        {
                                            "task_id": "ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦",
                                            "priority": 1,
                                            "fixing_type": "Add_File æˆ– Change_File",
                                            "which_file_to_fix": "å…·ä½“æ–‡ä»¶è·¯å¾„",
                                            "specific_location": "å…·ä½“ä½ç½®ï¼ˆè¡Œå·ã€å‡½æ•°åç­‰ï¼‰",
                                            "fixing_plan_in_detail": "è¯¦ç»†çš„ä¿®å¤è®¡åˆ’",
                                            "raw_code": "éœ€è¦ä¿®æ”¹çš„åŸå§‹ä»£ç ç‰‡æ®µ",

                                            "dependencies": ["ä¾èµ–çš„å…¶ä»–ä»»åŠ¡ID"],
                                        }
                                    ],
                                }
                                """

            # æ„å»ºåŒ…å«å®Œæ•´æ–‡ä»¶å†…å®¹çš„ç”¨æˆ·æç¤ºè¯
            user_prompt = self._build_deep_analysis_prompt(stdout, indexed_repo_data, file_contents, expected_behavior)
            # print('==================user_prompt====================')
            # print(user_prompt)
            # print('==================user_prompt====================')
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            print("ğŸ”„ æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œæ·±åº¦åˆ†æ...")
            print(f"ğŸ“Š è¾“å…¥å¤§å°: {len(user_prompt)} å­—ç¬¦")
            
            response = self.call_llm(messages, max_tokens=16384, temperature=0.3)
            
            print(f"âœ… LLMè°ƒç”¨å®Œæˆï¼Œå“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
            # print('==================LLMçš„è¿”å›å†…å®¹====================')
            # print(f"LLMçš„è¿”å›å†…å®¹: {response}")
            # print('==================LLMçš„è¿”å›å†…å®¹====================')
            print('\n\n')
            # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯å“åº”
            if response.startswith("è°ƒç”¨å¤±è´¥:") or response.startswith("LLMè°ƒç”¨å¼‚å¸¸:"):
                print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {response}")
                return self._generate_fallback_result(f"LLMè°ƒç”¨å¤±è´¥: {response}")
            
            # è°ƒè¯•ï¼šæ‰“å°LLMåŸå§‹å“åº”çš„å¼€å¤´å’Œç»“å°¾
            # print(f"\nğŸ” LLMå“åº”é¢„è§ˆ:")
            # print("=" * 50)
            # print("å¼€å¤´:")
            # print(response[:500] + "..." if len(response) > 500 else response)
            # if len(response) > 1000:
            #     print("\nç»“å°¾:")
            #     print("..." + response[-500:])
            # print("=" * 50)
            
            # è§£æJSONå“åº”
            print("ğŸ”„ æ­£åœ¨è§£æJSONå“åº”...")
            result = parse_json_response(response)
            
            # è°ƒè¯•ï¼šæ‰“å°è§£æç»“æœ
            print(f"\nğŸ“‹ è§£æç»“æœ:")
            print(f"tasksæ•°é‡: {len(result.get('tasks', []))}")
            if result.get('tasks'):
                for i, task in enumerate(result['tasks'][:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªä»»åŠ¡
                    print(f"  ä»»åŠ¡{i+1}: {task.get('task_id', 'unknown')} - {task.get('fixing_type', 'unknown')}")
            elif result.get('partial_tasks'):
                partial_tasks = result.get('partial_tasks', [])
                print(f"âš ï¸ ä½¿ç”¨æ¢å¤çš„éƒ¨åˆ†ä»»åŠ¡: {len(partial_tasks)} ä¸ª")
                for i, task in enumerate(partial_tasks[:2]):
                    print(f"  æ¢å¤ä»»åŠ¡{i+1}: {task.get('task_id', 'recovered')} - {task.get('fixing_type', 'unknown')}")
            else:
                print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•ä»»åŠ¡")
                if 'error' in result:
                    print(f"è§£æé”™è¯¯: {result['error']}")
                if 'raw_response' in result:
                    print(f"åŸå§‹å“åº”é•¿åº¦: {len(result['raw_response'])} å­—ç¬¦")
            print("=" * 50)
            
            # éªŒè¯å’Œå¢å¼ºä»»åŠ¡
            if "tasks" in result:
                result["tasks"] = validate_and_enhance_tasks(result["tasks"])
            elif "partial_tasks" in result:
                # å¦‚æœJSONè§£æå¤±è´¥ä½†æå–åˆ°äº†éƒ¨åˆ†ä»»åŠ¡ï¼Œä½¿ç”¨è¿™äº›ä»»åŠ¡
                print("ğŸ”„ ä½¿ç”¨ä»æˆªæ–­å“åº”ä¸­æ¢å¤çš„éƒ¨åˆ†ä»»åŠ¡")
                result["tasks"] = validate_and_enhance_tasks(result["partial_tasks"])
                # æ ‡è®°è¿™æ˜¯ä¸€ä¸ªéƒ¨åˆ†æ¢å¤çš„ç»“æœ
                result["partial_recovery"] = True
            else:
                # å¦‚æœå®Œå…¨æ²¡æœ‰ä»»åŠ¡ï¼Œç›´æ¥æŠ¥å‘Šå¤±è´¥
                print("âš ï¸ å®Œå…¨æ²¡æœ‰ä»»åŠ¡ï¼ŒLLMåˆ†æå¤±è´¥")
                result["tasks"] = []
            
            return result
            
        except Exception as e:
            return {
                "root_cause_analysis": f"æ·±åº¦åˆ†æå¤±è´¥: {str(e)}",
                "impact_analysis": "æ— æ³•åˆ†æå½±å“èŒƒå›´",
                "tasks": [],
                "error": str(e)
            }

    def _build_deep_analysis_prompt(self, stdout: str, indexed_repo_data: Dict[str, Any], 
                                   file_contents: Dict[str, Any], expected_behavior: str = None) -> str:
        """æ„å»ºé”™è¯¯é©±åŠ¨çš„æ·±åº¦åˆ†ææç¤ºè¯"""
        
        expected_behavior_section = ""
        if expected_behavior:
            expected_behavior_section = f"""
        æœŸæœ›çš„ç¨‹åºè¡Œä¸º: {expected_behavior}
        
        è¯·åŸºäºæœŸæœ›è¡Œä¸ºæ¥è®¾è®¡ä¿®å¤æ–¹æ¡ˆï¼Œç¡®ä¿ä¿®å¤åçš„ç¨‹åºèƒ½å¤ŸæŒ‰ç…§æœŸæœ›è¡Œä¸ºè¿è¡Œã€‚
        """
        
        prompt = f"""
        ç¨‹åºé”™è¯¯è¯¦æƒ…:{stdout}
        è¿™ä¸ªé”™è¯¯å¯¼è‡´ç¨‹åºæ— æ³•æ­£å¸¸è¿è¡Œï¼Œéœ€è¦ç«‹å³ä¿®å¤ï¼è¯·ç«‹å³åˆ†æä¸Šè¿°é”™è¯¯å¹¶æä¾›ä¿®å¤æ–¹æ¡ˆã€‚
        {expected_behavior_section}

        é¡¹ç›®å®Œæ•´ç´¢å¼•ä¿¡æ¯
        ```json
        {json.dumps(indexed_repo_data, indent=2, ensure_ascii=False)}
        ```

        ## ç›¸å…³æ–‡ä»¶åˆ†æ

        """

        
        # æ·»åŠ æ–‡ä»¶å†…å®¹ï¼Œä½†é™ä½è§†è§‰æƒé‡
        for file_path, file_info in file_contents.items():
            if "content" in file_info:
                prompt += f"""### ğŸ“„ {file_path} ({file_info['lines']}è¡Œ)
                            *åˆ†æåŸå› *: {file_info['reason']}
                            *ä¼˜å…ˆçº§*: {file_info['priority']}

                            ```python
                            {file_info['content']}
                            ```

                            """
            else:
                prompt += f"""### âŒ {file_path}
                                *é”™è¯¯*: {file_info.get('error', 'æœªçŸ¥é”™è¯¯')}
                                *åŸå› *: {file_info['reason']}

                                """
        
        prompt += f"""
                    # ğŸ“‹ TASK GENERATION REQUIREMENTS

                    åŸºäºä¸Šè¿°é”™è¯¯åˆ†æå’Œä»£ç ä¸Šä¸‹æ–‡ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚ç”Ÿæˆä¿®å¤ä»»åŠ¡ï¼š
                    1. è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š
                    {{
                        "root_cause_analysis": "é”™è¯¯çš„æ ¹æœ¬åŸå› åˆ†æ",
                        "impact_analysis": "é”™è¯¯å½±å“èŒƒå›´å’Œä¼ æ’­è·¯å¾„åˆ†æ",
                        "tasks": [
                            {{
                                "task_id": "ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦",
                                "priority": 1,
                                "fixing_type": "Add_File æˆ– Change_File",
                                "which_file_to_fix": "å…·ä½“æ–‡ä»¶è·¯å¾„",
                                "specific_location": "å…·ä½“ä½ç½®ï¼ˆè¡Œå·ã€å‡½æ•°åç­‰ï¼‰",
                                "fixing_plan_in_detail": "è¯¦ç»†çš„ä¿®å¤è®¡åˆ’",
                                "raw_code": "éœ€è¦ä¿®æ”¹çš„åŸå§‹ä»£ç ç‰‡æ®µ",

                                "dependencies": ["ä¾èµ–çš„å…¶ä»–ä»»åŠ¡ID"],
                            }}
                        ],
                    }}
                    2. which_file_to_fixå¿…é¡»è¿”å›å‡†ç¡®çš„æ–‡ä»¶åœ°å€ï¼Œä¸è¦è¿”å›æ–‡ä»¶åï¼Œè¦è¿”å›æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
                    3. fixing_plan_in_detailå¿…é¡»è¿”å›è¯¦ç»†çš„ä¿®å¤è®¡åˆ’ï¼ŒåŒ…æ‹¬ä¿®æ”¹çš„ä»£ç ç‰‡æ®µï¼Œä»¥åŠä¿®æ”¹çš„åŸå› 
                    4. raw_codeå¿…é¡»è¿”å›éœ€è¦ä¿®æ”¹çš„åŸå§‹ä»£ç ç‰‡æ®µ
                    5. dependencieså¿…é¡»è¿”å›ä¾èµ–çš„å…¶ä»–ä»»åŠ¡ID
                    6. é™¤äº†åˆ†æç»ˆç«¯è¾“å‡ºè§£å†³ç›´æ¥çš„é—®é¢˜ä»¥å¤–ï¼Œå¦‚æœä½ çœ‹åˆ°ä»£ç è¿˜æœ‰å…¶ä»–çš„BUGæ—¶ï¼Œä½ è¿”å›çš„taské‡Œé¢ä¹Ÿåº”è¯¥åŒ…å«ä¿®æ”¹å…¶ä»–BUGçš„task
                    è¯·ç°åœ¨å¼€å§‹åˆ†æå¹¶ç”Ÿæˆç¬¦åˆè¦æ±‚çš„ä¿®å¤ä»»åŠ¡ï¼"""

        return prompt



    def _summarize_call_graph(self, function_dependencies: Dict) -> Dict:
        """æ€»ç»“å‡½æ•°è°ƒç”¨å›¾ä¿¡æ¯"""
        call_graph = function_dependencies.get("call_graph", {})
        stats = function_dependencies.get("statistics", {})
        
        return {
            "total_functions": stats.get("total_functions", 0),
            "total_files": stats.get("total_files", 0),
            "has_call_relationships": bool(call_graph.get("nodes") or call_graph.get("edges"))
        }

    def _try_fix_truncated_json(self, json_str: str) -> str:
        """å°è¯•ä¿®å¤è¢«æˆªæ–­çš„JSON"""
        try:
            # æŸ¥æ‰¾æœ€åä¸€ä¸ªå®Œæ•´çš„ä»»åŠ¡
            if '"tasks":' in json_str and '[' in json_str:
                # æ‰¾åˆ°tasksæ•°ç»„çš„å¼€å§‹
                tasks_start = json_str.find('"tasks":')
                array_start = json_str.find('[', tasks_start)
                
                # ç»Ÿè®¡å¤§æ‹¬å·ï¼Œæ‰¾åˆ°æœ€åä¸€ä¸ªå®Œæ•´çš„ä»»åŠ¡å¯¹è±¡
                brace_count = 0
                last_complete_pos = array_start
                
                for i, char in enumerate(json_str[array_start:], array_start):
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            last_complete_pos = i + 1
                
                # æ„é€ ä¿®å¤åçš„JSON
                if last_complete_pos > array_start:
                    before_tasks = json_str[:array_start+1]
                    tasks_part = json_str[array_start+1:last_complete_pos]
                    
                    # æ„é€ å®Œæ•´çš„JSON
                    fixed = f'{before_tasks}{tasks_part}],"prevention_recommendations":"ä¿®å¤è¢«æˆªæ–­çš„å“åº”","testing_recommendations":"æ·»åŠ ç›¸å…³æµ‹è¯•"}}'
                    return fixed
                    
        except Exception:
            pass
        
        return None

    def _extract_partial_tasks(self, response: str) -> List[Dict]:
        """ä»å“åº”ä¸­æå–éƒ¨åˆ†ä»»åŠ¡ä¿¡æ¯ï¼Œå³ä½¿JSONä¸å®Œæ•´"""
        tasks = []
        try:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ä»»åŠ¡IDå’ŒåŸºæœ¬ä¿¡æ¯
            
            # æŸ¥æ‰¾task_idæ¨¡å¼
            task_id_pattern = r'"task_id":\s*"([^"]+)"'
            task_ids = re.findall(task_id_pattern, response)
            
            # æŸ¥æ‰¾fixing_typeæ¨¡å¼  
            fixing_type_pattern = r'"fixing_type":\s*"([^"]+)"'
            fixing_types = re.findall(fixing_type_pattern, response)
            
            # æŸ¥æ‰¾æ–‡ä»¶è·¯å¾„æ¨¡å¼
            file_pattern = r'"which_file_to_fix":\s*"([^"]+)"'
            files = re.findall(file_pattern, response)
            
            # ç»„åˆæ‰¾åˆ°çš„ä¿¡æ¯
            max_len = max(len(task_ids), len(fixing_types), len(files))
            for i in range(max_len):
                task = {
                    "task_id": task_ids[i] if i < len(task_ids) else f"partial_task_{i+1}",
                    "priority": i + 1,
                    "fixing_type": fixing_types[i] if i < len(fixing_types) else "Change_File",
                    "which_file_to_fix": files[i] if i < len(files) else "",
                    "fixing_plan_in_detail": "ä»æˆªæ–­å“åº”ä¸­æ¢å¤çš„éƒ¨åˆ†ä»»åŠ¡",
                    "raw_code": "",
                    "dependencies": [],
                    "estimated_impact": "æ— æ³•ç¡®å®š",
                    "partial_recovery": True
                }
                tasks.append(task)
                
        except Exception:
            pass
            
        return tasks

    def process(self, input_data: Any) -> Any:
        """
        å¤„ç†è¾“å…¥æ•°æ®ï¼ˆå®ç°åŸºç±»æŠ½è±¡æ–¹æ³•ï¼‰
        
        å‚æ•°:
            input_data: è¾“å…¥æ•°æ®ï¼ŒåŒ…å«stdoutå’Œrepo_path
        
        è¿”å›:
            dict: å¤šä»»åŠ¡åˆ†æç»“æœ
        """
        if isinstance(input_data, dict):
            stdout = input_data.get("stdout", "")
            repo_path = input_data.get("repo_path", "")
            indexed_repo = input_data.get("indexed_repo")
            return self.analyze_error(stdout, repo_path, indexed_repo)
        else:
            return self._generate_fallback_result("æ— æ•ˆçš„è¾“å…¥æ•°æ®æ ¼å¼")

    def _get_or_create_repo_index(self, repo_path: str) -> Dict[str, Any]:
        """
        è·å–æˆ–åˆ›å»ºä»£ç åº“ç´¢å¼• - ä½¿ç”¨åŸºç¡€ç»“æ„åˆ†æå™¨
        
        å‚æ•°:
            repo_path (str): ä»£ç åº“è·¯å¾„
        
        è¿”å›:
            dict: ä»£ç åº“ç´¢å¼•æ•°æ®ï¼ŒåŒ…å«åŸºç¡€ç»“æ„ä¿¡æ¯
        """
        try:
            # å°è¯•å¯¼å…¥åˆ†æå·¥å…·
            import sys
            sys.path.append(str(Path(__file__).parent.parent))
            
            from core_modules.simple_structure_analyzer import SimpleStructureAnalyzer
            
            # åŸºç¡€ç»“æ„åˆ†æ
            structure_analyzer = SimpleStructureAnalyzer(repo_path)
            structure_result = structure_analyzer.analyze_project()
            
            # æ„å»ºç»“æœ
            combined_result = {
                # æ¥è‡ªSimpleStructureAnalyzerçš„åŸºç¡€ä¿¡æ¯
                "project_name": structure_result.get("project_name", ""),
                "project_path": structure_result.get("project_path", ""),
                "directory_structure": structure_result.get("directory_structure", []),
                "files": structure_result.get("files", {}),
                
                # æä¾›ç©ºçš„ä¾èµ–ä¿¡æ¯ä»¥ä¿æŒå…¼å®¹æ€§
                "function_dependencies": {
                    "call_graph": {},
                    "function_details": {},
                    "statistics": {},
                    "analysis_timestamp": ""
                },
                
                # åˆ†æå…ƒä¿¡æ¯
                "analysis_info": {
                    "structure_analyzer": "SimpleStructureAnalyzer",
                    "dependency_analyzer": "None",
                    "has_dependency_analysis": False,
                    "analyzer_timestamp": get_current_timestamp()
                }
            }
            
            return combined_result
            
        except Exception as e:
            # å¦‚æœåˆ†æå·¥å…·ä¸å¯ç”¨ï¼Œç´§æ€¥æŠ¥é”™
            raise Exception(f"ç´¢å¼•ç”Ÿæˆå¤±è´¥: {str(e)}")
    
    def _extract_file_from_error(self, stdout: str, repo_path: str) -> str:
        """ä»é”™è¯¯ä¿¡æ¯ä¸­æå–æ–‡ä»¶è·¯å¾„"""
        
        # å¯»æ‰¾å¸¸è§çš„æ–‡ä»¶è·¯å¾„æ¨¡å¼
        patterns = [
            r'File "([^"]+)"',
            r"File '([^']+)'",
            r'in ([a-zA-Z_][a-zA-Z0-9_/\\]*\.py)',
            r'([a-zA-Z_][a-zA-Z0-9_/\\]*\.py)'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, stdout)
            if matches:
                file_path = matches[0]
                # è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
                if repo_path in file_path:
                    return file_path.replace(repo_path, '').lstrip('/')
                return file_path
        
        return "" 

    # def _read_files_completely(self, files_to_read: List[Dict], repo_path: str) -> Dict[str, Any]:
    #     """
    #     é˜¶æ®µ2: å®Œæ•´è¯»å–æ‰€æœ‰æ ‡è¯†çš„æ–‡ä»¶å†…å®¹
        
    #     å‚æ•°:
    #         files_to_read (list): éœ€è¦è¯»å–çš„æ–‡ä»¶åˆ—è¡¨
    #         repo_path (str): ä»£ç åº“æ ¹è·¯å¾„
        
    #     è¿”å›:
    #         dict: æ–‡ä»¶è¯»å–ç»“æœ
    #     """
    #     file_contents = {}
    #     reading_summary = {
    #         "total_files": len(files_to_read),
    #         "successful_reads": 0,
    #         "failed_reads": 0,
    #         "filtered_out": 0,
    #         "total_lines": 0,
    #         "total_size": 0
    #     }
        
    #     repo_path = Path(repo_path).resolve()
        
    #     for file_info in files_to_read:
    #         file_path = file_info["file_path"]
            
    #         # è¿‡æ»¤ä¸åº”è¯¥è¯»å–çš„æ–‡ä»¶
    #         if file_tools.should_filter_file(file_path):
    #             print(f"ğŸš« å·²è¿‡æ»¤: {file_path} (ä¸åº”è¯¥åˆ†æçš„æ–‡ä»¶)")
    #             reading_summary["filtered_out"] += 1
    #             continue
            
    #         full_path = repo_path / file_path
            
    #         try:
    #             # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    #             if not full_path.exists():
    #                 file_contents[file_path] = {
    #                     "error": "æ–‡ä»¶ä¸å­˜åœ¨",
    #                     "reason": file_info.get("reason", ""),
    #                     "priority": file_info.get("priority", "medium")
    #                 }
    #                 reading_summary["failed_reads"] += 1
    #                 continue
                
    #             # è¯»å–å®Œæ•´æ–‡ä»¶å†…å®¹
    #             with open(full_path, 'r', encoding='utf-8') as f:
    #                 content = f.read()
                
    #             lines = content.count('\n') + 1
    #             size = len(content)
                
    #             file_contents[file_path] = {
    #                 "content": content,
    #                 "size": size,
    #                 "lines": lines,
    #                 "reason": file_info.get("reason", ""),
    #                 "priority": file_info.get("priority", "medium"),
    #                 "analysis_focus": file_info.get("analysis_focus", "æ•´ä½“ä»£ç é€»è¾‘"),
    #                 "encoding": "utf-8"
    #             }
                
    #             reading_summary["successful_reads"] += 1
    #             reading_summary["total_lines"] += lines
    #             reading_summary["total_size"] += size
                
    #             print(f"ğŸ“„ å·²è¯»å–: {file_path} ({lines}è¡Œ, {size}å­—ç¬¦)")
                
    #         except Exception as e:
    #             file_contents[file_path] = {
    #                 "error": f"è¯»å–å¤±è´¥: {str(e)}",
    #                 "reason": file_info.get("reason", ""),
    #                 "priority": file_info.get("priority", "medium")
    #             }
    #             reading_summary["failed_reads"] += 1
    #             print(f"âŒ è¯»å–å¤±è´¥: {file_path} - {str(e)}")
        
    #     if reading_summary["filtered_out"] > 0:
    #         print(f"ğŸ“‹ è¯»å–æ€»ç»“: {reading_summary['successful_reads']} æˆåŠŸ, {reading_summary['failed_reads']} å¤±è´¥, {reading_summary['filtered_out']} è¿‡æ»¤")
        
    #     return {
    #         "file_contents": file_contents,
    #         "reading_summary": reading_summary,
    #         "context_estimation": estimate_context_usage(file_contents)
    #     }

    # def _should_filter_file(self, file_path: str) -> bool:
    #     """
    #     åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿‡æ»¤æŸä¸ªæ–‡ä»¶
        
    #     å‚æ•°:
    #         file_path (str): æ–‡ä»¶è·¯å¾„
        
    #     è¿”å›:
    #         bool: Trueè¡¨ç¤ºåº”è¯¥è¿‡æ»¤ï¼ŒFalseè¡¨ç¤ºå¯ä»¥è¯»å–
    #     """
    #     # è§„èŒƒåŒ–è·¯å¾„
    #     path = Path(file_path)
    #     path_str = str(path).lower()
        
    #     # è¿‡æ»¤è§„åˆ™
    #     filter_patterns = [
    #         # è°ƒè¯•è¾“å‡ºç›®å½•
    #         "debug_output",
    #         "debug_report",
            
    #         # å¤‡ä»½æ–‡ä»¶
    #         ".backup",
    #         ".bak",
            
    #         # éšè—æ–‡ä»¶å’Œç›®å½•
    #         "/.",
            
    #         # ç¼–è¯‘è¾“å‡º
    #         "__pycache__",
    #         ".pyc",
    #         ".pyo",
    #         ".pyd",
            
    #         # ç‰ˆæœ¬æ§åˆ¶
    #         ".git",
    #         ".svn",
    #         ".hg",
            
    #         # IDE/ç¼–è¾‘å™¨æ–‡ä»¶
    #         ".vscode",
    #         ".idea",
    #         ".vs",
            
    #         # è™šæ‹Ÿç¯å¢ƒ
    #         "/venv",
    #         "/env",
    #         "/virtualenv",
            
    #         # ä¾èµ–ç›®å½•
    #         "node_modules",
            
    #         # æ—¥å¿—æ–‡ä»¶
    #         ".log",
    #         "logs/",
            
    #         # ä¸´æ—¶æ–‡ä»¶
    #         ".tmp",
    #         ".temp",
    #         "~$",
            
    #         # ç³»ç»Ÿæ–‡ä»¶
    #         ".ds_store",
    #         "thumbs.db",
            
    #         # ä¿®æ”¹å†å²æ–‡ä»¶
    #         "modification_history",
            
    #         # æµ‹è¯•è¦†ç›–ç‡
    #         ".coverage",
    #         "htmlcov",
            
    #         # æ‰“åŒ…æ–‡ä»¶
    #         ".egg-info",
    #         "dist/",
    #         "build/"
    #     ]
        
    #     # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•è¿‡æ»¤æ¨¡å¼
    #     for pattern in filter_patterns:
    #         if pattern in path_str:
    #             return True
        
    #     # æ£€æŸ¥æ–‡ä»¶æ‰©å±•åï¼ˆåªè¯»å–ä»£ç æ–‡ä»¶ï¼‰
    #     allowed_extensions = {
    #         '.py', '.js', '.ts', '.java', '.cpp', '.c', '.h', '.hpp',
    #         '.cs', '.php', '.rb', '.go', '.rs', '.swift', '.kt',
    #         '.html', '.css', '.scss', '.vue', '.jsx', '.tsx',
    #         '.json', '.yaml', '.yml', '.xml', '.toml', '.ini',
    #         '.sql', '.sh', '.bat', '.ps1', '.md', '.txt', '.cfg'
    #     }
        
    #     if path.suffix and path.suffix.lower() not in allowed_extensions:
    #         return True
        
    #     return False




    # def _validate_and_enhance_tasks(self, tasks: List[Dict]) -> List[Dict]:
    #     """éªŒè¯å’Œå¢å¼ºä»»åŠ¡åˆ—è¡¨"""
    #     enhanced_tasks = []
        
    #     for i, task in enumerate(tasks):
    #         # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
    #         enhanced_task = {
    #             "task_id": task.get("task_id", f"task_{i+1}"),
    #             "priority": task.get("priority", i+1),
    #             "fixing_type": task.get("fixing_type", "Change_File"),
    #             "which_file_to_fix": task.get("which_file_to_fix", ""),
    #             "specific_location": task.get("specific_location", ""),
    #             "fixing_plan_in_detail": task.get("fixing_plan_in_detail", ""),
    #             "raw_code": task.get("raw_code", ""),
    #             "new_code": task.get("new_code", ""),
    #             "dependencies": task.get("dependencies", []),
    #             "estimated_impact": task.get("estimated_impact", ""),
    #             "risk_level": task.get("risk_level", "medium"),
    #             "verification_method": task.get("verification_method", "è¿è¡Œç¨‹åºéªŒè¯")
    #         }
            
    #         enhanced_tasks.append(enhanced_task)
        
    #     return enhanced_tasks

    # def _generate_execution_plan(self, tasks: List[Dict]) -> Dict[str, Any]:
    #     """ç”Ÿæˆä»»åŠ¡æ‰§è¡Œè®¡åˆ’"""
    #     if not tasks:
    #         return {
    #             "total_tasks": 0,
    #             "execution_order": [],
    #             "risk_assessment": "æ— ä»»åŠ¡éœ€è¦æ‰§è¡Œ"
    #         }
        
    #     # ç®€å•çš„æ‹“æ‰‘æ’åºï¼ŒæŒ‰ä¼˜å…ˆçº§å’Œä¾èµ–å…³ç³»æ’åº
    #     execution_order = []
    #     remaining_tasks = {task["task_id"]: task for task in tasks}
        
    #     while remaining_tasks:
    #         # æ‰¾åˆ°æ²¡æœ‰æœªæ»¡è¶³ä¾èµ–çš„ä»»åŠ¡
    #         ready_tasks = []
    #         for task_id, task in remaining_tasks.items():
    #             dependencies = task.get("dependencies", [])
    #             if all(dep_id not in remaining_tasks for dep_id in dependencies):
    #                 ready_tasks.append((task_id, task.get("priority", 999)))
            
    #         if not ready_tasks:
    #             # å­˜åœ¨å¾ªç¯ä¾èµ–ï¼ŒæŒ‰ä¼˜å…ˆçº§å¼ºåˆ¶æ‰§è¡Œ
    #             task_id = min(remaining_tasks.keys(), 
    #                         key=lambda tid: remaining_tasks[tid].get("priority", 999))
    #             ready_tasks = [(task_id, remaining_tasks[task_id].get("priority", 999))]
            
    #         # æŒ‰ä¼˜å…ˆçº§æ’åºå¹¶æ·»åŠ åˆ°æ‰§è¡Œé¡ºåº
    #         ready_tasks.sort(key=lambda x: x[1])
    #         for task_id, _ in ready_tasks:
    #             execution_order.append(task_id)
    #             del remaining_tasks[task_id]
        
    #     # é£é™©è¯„ä¼°
    #     high_risk_tasks = [t for t in tasks if t.get("risk_level") == "high"]
    #     risk_assessment = f"æ€»å…±{len(tasks)}ä¸ªä»»åŠ¡ï¼Œ{len(high_risk_tasks)}ä¸ªé«˜é£é™©ä»»åŠ¡"
        
    #     return {
    #         "total_tasks": len(tasks),
    #         "execution_order": execution_order,
    #         "risk_assessment": risk_assessment
    #     }

    # def _generate_fallback_result(self, error_message: str) -> Dict[str, Any]:
    #     """ç”Ÿæˆå¤‡ç”¨ç»“æœ"""
    #     return {
    #         "analysis_stages": {
    #             "file_identification": {"error": error_message},
    #             "file_reading": {"error": "æœªæ‰§è¡Œ"},
    #             "deep_analysis": {"error": "æœªæ‰§è¡Œ"}
    #         },
    #         "tasks": [{
    #             "task_id": "fallback_task",
    #             "priority": 1,
    #             "fixing_type": "Change_File",
    #             "which_file_to_fix": "",
    #             "fixing_plan_in_detail": f"åˆ†æå¤±è´¥: {error_message}",
    #             "raw_code": "",
    #             "dependencies": [],
    #             "estimated_impact": "æ— æ³•è¯„ä¼°"
    #         }],
    #         "execution_plan": {
    #             "total_tasks": 1,
    #             "execution_order": ["fallback_task"],
    #             "risk_assessment": "åˆ†æå¤±è´¥ï¼Œæ— æ³•è¯„ä¼°é£é™©"
    #         }
    #     }

    # def _estimate_context_usage(self, file_contents: Dict) -> Dict:
    #     """ä¼°ç®—ä¸Šä¸‹æ–‡ä½¿ç”¨é‡"""
    #     total_chars = sum(
    #         info.get("size", 0) for info in file_contents.values() 
    #         if "content" in info
    #     )
    #     total_lines = sum(
    #         info.get("lines", 0) for info in file_contents.values() 
    #         if "content" in info
    #     )
        
    #     # ç²—ç•¥ä¼°ç®—tokenæ•°ï¼ˆå‡è®¾4ä¸ªå­—ç¬¦=1ä¸ªtokenï¼‰
    #     estimated_tokens = total_chars // 4
        
    #     return {
    #         "total_characters": total_chars,
    #         "total_lines": total_lines,
    #         "estimated_tokens": estimated_tokens,
    #         "context_status": "acceptable" if estimated_tokens < 80000 else "large"
    #     }

    # def _parse_json_response(self, response: str) -> Dict:
    #     """è§£æLLMçš„JSONå“åº” - å¢å¼ºç‰ˆæœ¬ï¼Œæ›´robuståœ°å¤„ç†å„ç§æ ¼å¼é—®é¢˜"""
    #     try:
    #         # 1. é¦–å…ˆå°è¯•ç›´æ¥è§£æ
    #         try:
    #             return json.loads(response.strip())
    #         except json.JSONDecodeError:
    #             pass
            
    #         # 2. å°è¯•æå–JSONä»£ç å—
    #         if "```json" in response:
    #             json_start = response.find("```json") + 7
    #             json_end = response.find("```", json_start)
    #             if json_end == -1:
    #                 # æ²¡æœ‰ç»“æŸæ ‡è®°ï¼Œå¯èƒ½è¢«æˆªæ–­
    #                 json_str = response[json_start:].strip()
    #             else:
    #                 json_str = response[json_start:json_end].strip()
    #         elif "{" in response and "}" in response:
    #             # 3. å¯»æ‰¾ç¬¬ä¸€ä¸ª{åˆ°æœ€åä¸€ä¸ª}
    #             json_start = response.find("{")
    #             json_end = response.rfind("}") + 1
    #             json_str = response[json_start:json_end]
    #         else:
    #             json_str = response.strip()
            
    #         # 4. å°è¯•è§£ææå–çš„JSON
    #         try:
    #             return json.loads(json_str)
    #         except json.JSONDecodeError as e:
    #             # 5. å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤å¸¸è§é—®é¢˜
    #             print(f"âš ï¸ JSONè§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤: {str(e)}")
                
    #             # å°è¯•ä¿®å¤æˆªæ–­çš„JSON
    #             fixed_json = self._try_fix_truncated_json(json_str)
    #             if fixed_json:
    #                 try:
    #                     return json.loads(fixed_json)
    #                 except json.JSONDecodeError:
    #                     pass
                
    #             # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯ä½†åŒ…å«éƒ¨åˆ†æ•°æ®
    #             return {
    #                 "error": f"JSONè§£æå¤±è´¥: {str(e)}",
    #                 "raw_response": response,
    #                 "partial_tasks": self._extract_partial_tasks(response)
    #             }

    #     except Exception as e:
    #         return {
    #             "error": f"å“åº”å¤„ç†å¼‚å¸¸: {str(e)}",
    #             "raw_response": response
    #         }

        # def _get_current_timestamp(self) -> str:
    #     """è·å–å½“å‰æ—¶é—´æˆ³"""
    #     from datetime import datetime
    #     return datetime.now().isoformat()

    # def _get_basic_file_list(self, repo_path: str) -> list:
    #     """
    #     è·å–åŸºæœ¬çš„æ–‡ä»¶åˆ—è¡¨
        
    #     å‚æ•°:
    #         repo_path (str): ä»£ç åº“è·¯å¾„
        
    #     è¿”å›:
    #         list: æ–‡ä»¶åˆ—è¡¨
    #     """
    #     try:
    #         files = []
    #         for root, dirs, filenames in os.walk(repo_path):
    #             # æ’é™¤éšè—ç›®å½•
    #             dirs[:] = [d for d in dirs if not d.startswith('.')]
                
    #             level = root.replace(repo_path, '').count(os.sep)
    #             indent = '  ' * level
    #             rel_root = os.path.relpath(root, repo_path)
                
    #             if rel_root != '.':
    #                 files.append(f"{indent}{os.path.basename(root)}/")
                
    #             sub_indent = '  ' * (level + 1)
    #             for filename in filenames:
    #                 if not filename.startswith('.'):
    #                     files.append(f"{sub_indent}{filename}")
            
    #         return files
            
    #     except Exception as e:
    #         return [f"é”™è¯¯: æ— æ³•åˆ—å‡ºæ–‡ä»¶ - {str(e)}"]

'''
===========================================================
ç³»ç»Ÿæç¤ºè¯å¤‡ä»½ï¼š    def _deep_analysis_with_content
===========================================================

            system_prompt = """ğŸ¯ ä½ æ˜¯ä¸“ä¸šçš„ä»£ç é”™è¯¯ä¿®å¤ä¸“å®¶ã€‚ä½ çš„é¦–è¦ä¸”å”¯ä¸€ç›®æ ‡æ˜¯ï¼šè®©ç¨‹åºèƒ½å¤ŸæˆåŠŸè¿è¡Œã€‚

                                ## é”™è¯¯é©±åŠ¨çš„åˆ†æåŸåˆ™

                                **ä¼˜å…ˆçº§ç­–ç•¥**ï¼šç›´æ¥ä¿®å¤stdouté”™è¯¯ > é˜²å¾¡æ€§ç¼–ç¨‹ > ç³»ç»Ÿä¼˜åŒ– > æµ‹è¯•å¢å¼º

                                **é”™è¯¯ç±»å‹å¤„ç†ç­–ç•¥**ï¼š
                                ğŸ”¥ ä¸´ç•Œé”™è¯¯ï¼ˆç¨‹åºæ— æ³•å¯åŠ¨ï¼‰- ç«‹å³ä¿®å¤
                                â”œâ”€â”€ ImportError/ModuleNotFoundError â†’ åˆ›å»ºç¼ºå¤±æ¨¡å—æˆ–ä¿®å¤å¯¼å…¥è·¯å¾„
                                â”œâ”€â”€ SyntaxError â†’ ä¿®å¤è¯­æ³•é”™è¯¯
                                â”œâ”€â”€ IndentationError â†’ ä¿®å¤ç¼©è¿›é—®é¢˜
                                â””â”€â”€ FileNotFoundError â†’ åˆ›å»ºç¼ºå¤±æ–‡ä»¶æˆ–ä¿®å¤è·¯å¾„

                                âš ï¸ è¿è¡Œæ—¶é”™è¯¯ï¼ˆç¨‹åºå´©æºƒï¼‰- ä¼˜å…ˆå¤„ç†
                                â”œâ”€â”€ AttributeError â†’ ä¿®å¤å±æ€§è®¿é—®é”™è¯¯
                                â”œâ”€â”€ NameError â†’ ä¿®å¤æœªå®šä¹‰å˜é‡
                                â”œâ”€â”€ TypeError â†’ ä¿®å¤ç±»å‹é”™è¯¯
                                â””â”€â”€ ValueError â†’ ä¿®å¤å€¼é”™è¯¯

                                ğŸ› é€»è¾‘é”™è¯¯ï¼ˆç»“æœä¸æ­£ç¡®ï¼‰- åç»­ä¼˜åŒ–
                                â”œâ”€â”€ AssertionError â†’ ä¿®å¤æ–­è¨€é€»è¾‘
                                â”œâ”€â”€ IndexError â†’ ä¿®å¤ç´¢å¼•è®¿é—®
                                â””â”€â”€ ä¸šåŠ¡é€»è¾‘é”™è¯¯ â†’ æ”¹è¿›ç®—æ³•é€»è¾‘

                                ## å¼ºåˆ¶ä»»åŠ¡ç”Ÿæˆè§„åˆ™

                                **CRITICALè¦æ±‚**ï¼š
                                1. ç¬¬ä¸€ä¸ªä»»åŠ¡å¿…é¡»ï¼špriority=1ï¼Œç›´æ¥è§£å†³stdoutä¸­æ˜¾ç¤ºçš„å…·ä½“é”™è¯¯
                                2. ç¬¬ä¸€ä¸ªä»»åŠ¡å¿…é¡»ï¼šæä¾›èƒ½è®©ç¨‹åºç«‹å³è¿è¡Œçš„æœ€å°ä¿®å¤æ–¹æ¡ˆ
                                3. åç»­ä»»åŠ¡ï¼špriority=2+ï¼Œä¾èµ–äºæ ¸å¿ƒä¿®å¤ä»»åŠ¡çš„å®Œæˆ
                                4. æ¯ä¸ªä»»åŠ¡ï¼šå¿…é¡»è¯´æ˜å…·ä½“çš„éªŒè¯æ–¹æ³•

                                **ä»»åŠ¡åˆ†å±‚ç­–ç•¥**ï¼š
                                - Priority 1: ç›´æ¥ä¿®å¤stderr/stdoutä¸­çš„é”™è¯¯ï¼Œè®©ç¨‹åºèƒ½è¿è¡Œ
                                - Priority 2: éªŒè¯ä¿®å¤æ•ˆæœï¼Œç¡®ä¿ç¨‹åºç¨³å®š
                                - Priority 3: é˜²å¾¡æ€§ç¼–ç¨‹ï¼Œæ·»åŠ é”™è¯¯å¤„ç†
                                - Priority 4+: ä»£ç ä¼˜åŒ–ã€é‡æ„ã€æµ‹è¯•å¢å¼º

                                è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š
                                {
                                    "root_cause_analysis": "é”™è¯¯çš„æ ¹æœ¬åŸå› åˆ†æ",
                                    "impact_analysis": "é”™è¯¯å½±å“èŒƒå›´å’Œä¼ æ’­è·¯å¾„åˆ†æ",
                                    "tasks": [
                                        {
                                            "task_id": "ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦",
                                            "priority": 1,
                                            "fixing_type": "Add_File æˆ– Change_File",
                                            "which_file_to_fix": "å…·ä½“æ–‡ä»¶è·¯å¾„",
                                            "specific_location": "å…·ä½“ä½ç½®ï¼ˆè¡Œå·ã€å‡½æ•°åç­‰ï¼‰",
                                            "fixing_plan_in_detail": "è¯¦ç»†çš„ä¿®å¤è®¡åˆ’",
                                            "raw_code": "éœ€è¦ä¿®æ”¹çš„åŸå§‹ä»£ç ç‰‡æ®µ",
                                            "new_code": "å»ºè®®çš„æ–°ä»£ç ï¼ˆå¦‚æœé€‚ç”¨ï¼‰",
                                            "dependencies": ["ä¾èµ–çš„å…¶ä»–ä»»åŠ¡ID"],
                                            "estimated_impact": "ä¿®å¤çš„é¢„æœŸå½±å“",
                                            "risk_level": "low/medium/high",
                                            "verification_method": "å¦‚ä½•éªŒè¯ä¿®å¤æ˜¯å¦æˆåŠŸ"
                                        }
                                    ],
                                    "prevention_recommendations": "é˜²æ­¢ç±»ä¼¼é—®é¢˜çš„å»ºè®®",
                                    "testing_recommendations": "å»ºè®®æ·»åŠ çš„æµ‹è¯•"
                                }
                                ## åˆ†ææ‰§è¡Œè¦æ±‚
                                1. **é”™è¯¯èšç„¦**ï¼šç«‹å³è¯†åˆ«stdouté”™è¯¯ç±»å‹å’Œä¸¥é‡ç¨‹åº¦
                                2. **æœ€å°ä¿®å¤**ï¼šä¼˜å…ˆæä¾›èƒ½è®©ç¨‹åºè¿è¡Œçš„æœ€å°æ”¹åŠ¨
                                3. **æ¸è¿›æ”¹è¿›**ï¼šåœ¨ç¨‹åºèƒ½è¿è¡Œçš„åŸºç¡€ä¸Šå†è€ƒè™‘ä¼˜åŒ–
                                4. **éªŒè¯å¯¼å‘**ï¼šæ¯ä¸ªä¿®å¤éƒ½è¦å¯éªŒè¯å’Œå¯æµ‹è¯•"""

===========================================================
ç³»ç»Ÿæç¤ºè¯å¤‡ä»½ï¼š    def _deep_analysis_with_content
===========================================================
'''


if __name__ == "__main__":
    #æµ‹è¯•_get_or_create_repo_indexåŠŸèƒ½ï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯ï¼Œå¹¶ä¼˜é›…å±•ç¤ºJSONå†…å®¹ä½¿å…¶åœ¨ç»ˆç«¯çš„å¯è¯»æ€§å¼º
    # project_path = "/Users/wwchdemac/python_projects/debug_agent/test_input/webpage"
    # analyzer = AnalyzerAgent(project_path = project_path)
    # result = analyzer._get_or_create_repo_index(project_path)
    # print(json.dumps(result, indent=4, ensure_ascii=False))
    # #æœ€åç»Ÿè®¡ç»ˆç«¯æ‰“å°çš„å­—ç¬¦æ•°é‡ï¼Œè¦ä»”ç»†ç»Ÿè®¡JSONé‡Œé¢çš„æ‰€æœ‰å­—ç¬¦
    # print(f"ç»ˆç«¯æ‰“å°çš„å­—ç¬¦æ•°é‡: {len(json.dumps(result, indent=4, ensure_ascii=False))}")
    #æµ‹è¯•_build_deep_analysis_prompt

    # æµ‹è¯• _build_deep_analysis_prompt æ–¹æ³•çš„æ­£ç¡®æ–¹å¼
    project_path = "/Users/wwchdemac/python_projects/debug_agent/test_input/webpage"
    expected_behavior = "ç¨‹åºèƒ½å¤Ÿæ­£å¸¸è¿è¡Œï¼Œå¹¶ä¸”èƒ½å¤Ÿæ­£ç¡®æ˜¾ç¤ºç½‘é¡µ"
    # 1. åˆ›å»ºåˆ†æå™¨å®ä¾‹
    analyzer = AnalyzerAgent(project_path=project_path)

    # 2. å‡†å¤‡æ¨¡æ‹Ÿçš„é”™è¯¯è¾“å‡ºï¼ˆstdoutå‚æ•°ï¼‰
    stdout = """
    Traceback (most recent call last):
    File "/Users/wwchdemac/python_projects/debug_agent/test_input/webpage/backend/__init__.py", line 10, in <module>
        from .config import Config
    ImportError: attempted relative import with no known parent package
    """

    # 3. è·å–é¡¹ç›®ç´¢å¼•æ•°æ®
    indexed_repo_data = analyzer._get_or_create_repo_index(project_path)

    # 4. å‡†å¤‡æ–‡ä»¶å†…å®¹æ•°æ® (è¿™é‡Œéœ€è¦æ¨¡æ‹Ÿæˆ–è€…é€šè¿‡æ­£ç¡®çš„æ–¹å¼è·å–)
    # æ–¹å¼1: æ‰‹åŠ¨æ„å»ºæ–‡ä»¶å†…å®¹å­—å…¸
    # file_contents = {
    #     "backend/__init__.py": {
    #         "content": "æ–‡ä»¶å†…å®¹...",
    #         "lines": 24,
    #         "size": 625,
    #         "encoding": "utf-8"
    #     },
    #     "backend/config.py": {
    #         "content": "æ–‡ä»¶å†…å®¹...", 
    #         "lines": 70,
    #         "size": 2200,
    #         "encoding": "utf-8"
    #     }
    # }

    # æ–¹å¼2: æˆ–è€…é€šè¿‡å®Œæ•´çš„åˆ†ææµç¨‹è·å–
    from agents.utils.file_tools import read_files_completely

    print('==================analyzer._identify_relevant_files====================')
    # é¦–å…ˆè¯†åˆ«ç›¸å…³æ–‡ä»¶
    file_identification_result = analyzer._identify_relevant_files(stdout, indexed_repo_data, expected_behavior)

    print('\n\n')
    files_to_read = file_identification_result.get("files_to_read", [])
    
    print('==================files_to_read====================')
    # ç„¶åè¯»å–æ–‡ä»¶å†…å®¹
    file_reading_result = read_files_completely(files_to_read, project_path)
    file_contents = file_reading_result.get("file_contents", {})
    print('==================file_reading_result====================')
    # 5. ç°åœ¨å¯ä»¥æ­£ç¡®è°ƒç”¨æ–¹æ³•äº†
    result = analyzer._build_deep_analysis_prompt(stdout, indexed_repo_data, file_contents, expected_behavior)
    #æˆ‘è¿˜å¸Œæœ›çœ‹çœ‹å°†æç¤ºè¯å‘é€ä¹‹åLLMçš„è¿”å›å†…å®¹ï¼Œ_deep_analysis_with_content
    result = analyzer._deep_analysis_with_content(stdout, indexed_repo_data, file_contents, expected_behavior)
    print('==================result====================')
    #ä¼˜é›…æ‰“å°JSON
    print(json.dumps(result, indent=4, ensure_ascii=False))
    print('==================result====================')


