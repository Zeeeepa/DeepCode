"""
è®ºæ–‡åˆ†æAgent

ç»§æ‰¿è‡ªAnalyzerAgentï¼Œä¸“é—¨ç”¨äºè®ºæ–‡å¤ç°çš„åˆ†æéœ€æ±‚ã€‚
é’ˆå¯¹è®ºæ–‡å¤ç°çš„5ä¸ªå±‚çº§ï¼ˆL0-L4ï¼‰æä¾›ä¸“é—¨çš„åˆ†æé€»è¾‘å’Œæç¤ºè¯ã€‚

ä¸»è¦åŠŸèƒ½:
- PaperAnalyzerAgent: ä¸“é—¨çš„è®ºæ–‡åˆ†æAgent
- analyze_paper_level: é’ˆå¯¹ç‰¹å®šå±‚çº§çš„åˆ†ææ–¹æ³•
- æ¯ä¸ªå±‚çº§æœ‰ä¸“é—¨çš„åˆ†æç­–ç•¥å’Œæç¤ºè¯
- å®Œå…¨ä¸å½±å“åŸæœ‰çš„AnalyzerAgentåŠŸèƒ½
"""

import json
from typing import Dict, Any, List
from .analyzer_agent import AnalyzerAgent
from .utils import (
    parse_json_response, 
    get_current_timestamp,
    create_repo_index,
    generate_execution_plan,
    get_colored_logger,
    log_detailed,
    log_llm_call,
    log_operation_start,
    log_operation_success,
    log_operation_error,
    load_paper_guide
)


class PaperAnalyzerAgent(AnalyzerAgent):
    """
    è®ºæ–‡åˆ†æAgent
    
    ç»§æ‰¿è‡ªAnalyzerAgentï¼Œä¸“é—¨ç”¨äºè®ºæ–‡å¤ç°çš„åˆ†æã€‚
    é’ˆå¯¹5ä¸ªå¤ç°å±‚çº§æä¾›ä¸“é—¨çš„åˆ†æé€»è¾‘ï¼š
    - L0: ç¯å¢ƒæ­å»ºåˆ†æ
    - L1: æ ¸å¿ƒç®—æ³•åˆ†æ  
    - L2: è®­ç»ƒæµç¨‹åˆ†æ
    - L3: å®éªŒå¤ç°åˆ†æ
    - L4: ç»“æœå¯¹é½åˆ†æ
    """
    
    def __init__(self, **kwargs):
        """
        åˆå§‹åŒ–è®ºæ–‡åˆ†æAgent
        
        å‚æ•°:
            **kwargs: ä¼ é€’ç»™çˆ¶ç±»AnalyzerAgentçš„å‚æ•°
        """
        super().__init__(**kwargs)
        
        # åˆå§‹åŒ–ä¸“é—¨çš„å½©è‰²æ—¥å¿—è®°å½•å™¨
        self.paper_logger = get_colored_logger("PaperAnalyzer")
        self.paper_logger.info("ğŸ¯ PaperAnalyzerAgent åˆå§‹åŒ–å®Œæˆ")
        
        # è®ºæ–‡å¤ç°å±‚çº§çš„åˆ†æç­–ç•¥
        self.level_strategies = {
            "L0": {
                "name": "ç¯å¢ƒæ­å»º",
                "focus": ["ä¾èµ–å®‰è£…", "ç¯å¢ƒé…ç½®", "æ•°æ®å‡†å¤‡", "åŸºç¡€è¿è¡Œèƒ½åŠ›"],
                "max_tasks": 5,
                "priority_keywords": ["import", "requirement", "dependency", "environment", "setup", "install"]
            },
            "L1": {
                "name": "æ ¸å¿ƒç®—æ³•", 
                "focus": ["ç®—æ³•å®ç°", "æ¨¡å‹æ¶æ„", "å…³é”®å…¬å¼", "æ ¸å¿ƒç»„ä»¶"],
                "max_tasks": 6,
                "priority_keywords": ["algorithm", "model", "network", "layer", "function", "class", "formula"]
            },
            "L2": {
                "name": "è®­ç»ƒæµç¨‹",
                "focus": ["è®­ç»ƒå¾ªç¯", "æŸå¤±è®¡ç®—", "ä¼˜åŒ–å™¨", "æ•°æ®æµæ°´çº¿", "æ¨ç†æµç¨‹"],
                "max_tasks": 5,
                "priority_keywords": ["train", "loss", "optimizer", "forward", "backward", "epoch", "batch"]
            },
            "L3": {
                "name": "å®éªŒå¤ç°",
                "focus": ["å®éªŒè„šæœ¬", "è¯„ä¼°æŒ‡æ ‡", "æ•°æ®é›†å¤„ç†", "ç»“æœè¾“å‡º"],
                "max_tasks": 4,
                "priority_keywords": ["experiment", "eval", "test", "metric", "dataset", "benchmark"]
            },
            "L4": {
                "name": "ç»“æœå¯¹é½", 
                "focus": ["ç»“æœä¼˜åŒ–", "å‚æ•°è°ƒæ•´", "æ€§èƒ½æ”¹è¿›", "æ•°å€¼ä¸€è‡´æ€§"],
                "max_tasks": 4,
                "priority_keywords": ["performance", "accuracy", "result", "output", "metric", "score"]
            }
        }
    
    def analyze_paper_level(self, 
                           level_code: str,
                           level_description: str,
                           paper_guide: str,
                           repo_path: str,
                           additional_content: str = "",
                           target_metrics: Dict[str, Any] = None,
                           iteration: int = 1) -> Dict[str, Any]:
        """
        åˆ†æè®ºæ–‡å¤ç°çš„ç‰¹å®šå±‚çº§
        
        å‚æ•°:
            level_code (str): å±‚çº§ä»£ç  (L0, L1, L2, L3, L4)
            level_description (str): å±‚çº§æè¿°
            paper_guide (str): è®ºæ–‡å¤ç°æŒ‡å—
            repo_path (str): ä»£ç åº“è·¯å¾„
            additional_content (str): è¡¥å……ä¿¡æ¯å†…å®¹ï¼ˆå¯é€‰ï¼‰
            target_metrics (dict): ç›®æ ‡æŒ‡æ ‡
            iteration (int): å½“å‰è¿­ä»£æ¬¡æ•°
        
        è¿”å›:
            dict: å±‚çº§åˆ†æç»“æœ
        """
        try:
            # å¼€å§‹å±‚çº§åˆ†æ
            log_operation_start(self.paper_logger, f"{level_code}å±‚çº§åˆ†æ")
            self.paper_logger.info(f"ğŸ” å¼€å§‹{level_code}å±‚çº§åˆ†æ...")
            
            # è·å–å±‚çº§ç­–ç•¥
            strategy = self.level_strategies.get(level_code, {})
            level_name = strategy.get("name", level_code)
            focus_areas = strategy.get("focus", [])
            max_tasks = strategy.get("max_tasks", 5)
            
            # è®°å½•åˆ†æé…ç½®
            analysis_config = {
                "å±‚çº§ä»£ç ": level_code,
                "å±‚çº§åç§°": level_name,
                "ä¸“æ³¨é¢†åŸŸ": ", ".join(focus_areas),
                "æœ€å¤§ä»»åŠ¡æ•°": max_tasks,
                "è¿­ä»£æ¬¡æ•°": iteration
            }
            log_detailed(self.paper_logger, "ğŸ“‹ åˆ†æé…ç½®", analysis_config)
            
            # å¤„ç†paper_guideå‚æ•°ï¼ˆæ”¯æŒmarkdownæ–‡ä»¶è·¯å¾„æˆ–ç›´æ¥å†…å®¹ï¼‰
            log_operation_start(self.paper_logger, "Paper Guideå¤„ç†")
            processed_paper_guide = load_paper_guide(paper_guide)
            if not processed_paper_guide:
                self.paper_logger.warning("âš ï¸ Paper Guideä¸ºç©ºï¼Œå°†ä½¿ç”¨ç©ºæŒ‡å—è¿›è¡Œåˆ†æ")
            else:
                guide_info = {
                    "å†…å®¹é•¿åº¦": f"{len(processed_paper_guide)} å­—ç¬¦",
                    "è¡Œæ•°": len(processed_paper_guide.splitlines()),
                    "æ˜¯å¦ä»æ–‡ä»¶åŠ è½½": paper_guide.strip().endswith(('.md', '.markdown'))
                }
                log_detailed(self.paper_logger, "ğŸ“„ Paper Guideä¿¡æ¯", guide_info)
            log_operation_success(self.paper_logger, "Paper Guideå¤„ç†")
            
            # åˆ›å»ºæˆ–è·å–ä»£ç åº“ç´¢å¼•
            log_operation_start(self.paper_logger, "ä»£ç åº“ç´¢å¼•åˆ›å»º/è·å–")
            
            indexed_repo_data = self._get_or_create_repo_index(repo_path)
            if not indexed_repo_data:
                log_operation_error(self.paper_logger, "ä»£ç åº“ç´¢å¼•åˆ›å»º", "ç´¢å¼•æ•°æ®ä¸ºç©º")
                return self._generate_fallback_result(level_code, "æ— æ³•åˆ›å»ºä»£ç åº“ç´¢å¼•")
            
            log_operation_success(self.paper_logger, "ä»£ç åº“ç´¢å¼•åˆ›å»º/è·å–")
            
            # è®°å½•ç´¢å¼•ç»Ÿè®¡
            # index_stats = {
            #     "é¡¹ç›®åç§°": indexed_repo_data.get("project_name", "æœªçŸ¥"),
            #     "æ–‡ä»¶æ•°é‡": len(indexed_repo_data.get("files", {})),
            #     "ç›®å½•æ•°é‡": len(indexed_repo_data.get("directory_structure", [])),
            #     "æ˜¯å¦æœ‰ä¾èµ–åˆ†æ": indexed_repo_data.get("analysis_info", {}).get("has_dependency_analysis", "å¦")
            # }
            # log_detailed(self.paper_logger, "ğŸ“Š ä»£ç åº“ç´¢å¼•ç»Ÿè®¡", index_stats)
            
            # æ„å»ºå±‚çº§ä¸“é—¨çš„ç³»ç»Ÿæç¤ºè¯
            system_prompt = self._build_level_system_prompt(level_code, level_name, focus_areas, max_tasks)
            
            # æ„å»ºç”¨æˆ·æç¤ºè¯
            user_prompt = self._build_level_user_prompt(
                level_code, level_description, processed_paper_guide, 
                indexed_repo_data, additional_content, target_metrics, iteration
            )
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            # è®°å½•LLMè°ƒç”¨ä¿¡æ¯
            llm_info = {
                "ç³»ç»Ÿæç¤ºè¯é•¿åº¦": f"{len(system_prompt)} å­—ç¬¦",
                "ç”¨æˆ·æç¤ºè¯é•¿åº¦": f"{len(user_prompt)} å­—ç¬¦",
                "æ€»è¾“å…¥é•¿åº¦": f"{len(system_prompt) + len(user_prompt)} å­—ç¬¦",
                "æœ€å¤§è¾“å‡ºToken": "16384",
                "æ¸©åº¦å‚æ•°": "0.3"
            }
            log_detailed(self.paper_logger, "ğŸ“¡ LLMè°ƒç”¨å‚æ•°", llm_info)
            
            log_operation_start(self.paper_logger, f"{level_code}å±‚çº§LLMåˆ†æ")
            self.paper_logger.info("ğŸ”„ æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œå±‚çº§åˆ†æ...")
            
            # ä½¿ç”¨ç°æœ‰çš„log_llm_callå‡½æ•°
            log_llm_call(self.paper_logger, "å±‚çº§åˆ†ææ¨¡å‹", 16384, len(user_prompt))
            
            # è°ƒç”¨LLM
            response = self.call_llm(messages, max_tokens=16384, temperature=0.3)
            log_operation_success(self.paper_logger, f"{level_code}å±‚çº§LLMåˆ†æ")
            
            # è®°å½•å“åº”ç»Ÿè®¡
            response_stats = {
                "å“åº”é•¿åº¦": f"{len(response)} å­—ç¬¦",
                "æ˜¯å¦åŒ…å«JSON": "æ˜¯" if "{" in response and "}" in response else "å¦"
            }
            log_detailed(self.paper_logger, "ğŸ“¥ LLMå“åº”ç»Ÿè®¡", response_stats)
            
            # è§£æå“åº”
            log_operation_start(self.paper_logger, "JSONå“åº”è§£æ")
            result = parse_json_response(response)
            
            if result:
                log_operation_success(self.paper_logger, "JSONå“åº”è§£æ")
                self.paper_logger.info("âœ… JSONè§£ææˆåŠŸ")
            else:
                log_operation_error(self.paper_logger, "JSONå“åº”è§£æ", "è§£æå¤±è´¥æˆ–ç»“æœä¸ºç©º")
                self.paper_logger.error("âŒ JSONè§£æå¤±è´¥")
            
            # éªŒè¯å’Œå¢å¼ºç»“æœ
            log_operation_start(self.paper_logger, "ä»»åŠ¡éªŒè¯å’Œå¢å¼º")
            
            if "tasks" in result and result["tasks"]:
                original_task_count = len(result["tasks"])
                # é™åˆ¶ä»»åŠ¡æ•°é‡
                result["tasks"] = result["tasks"][:max_tasks]
                
                if len(result["tasks"]) < original_task_count:
                    self.paper_logger.warning(f"âš ï¸  ä»»åŠ¡æ•°é‡ä» {original_task_count} é™åˆ¶ä¸º {len(result['tasks'])}")
                
                # è®°å½•ä»»åŠ¡ç±»å‹ç»Ÿè®¡
                task_types = {}
                for task in result["tasks"]:
                    fixing_type = task.get("fixing_type", "æœªçŸ¥")
                    task_types[fixing_type] = task_types.get(fixing_type, 0) + 1
                
                # ä¸ºæ¯ä¸ªä»»åŠ¡æ·»åŠ å±‚çº§ä¿¡æ¯
                for i, task in enumerate(result["tasks"]):
                    if "task_id" not in task:
                        task["task_id"] = f"{level_code}_task_{i+1}"
                    task["level_code"] = level_code
                    task["level_name"] = level_name
                    task["iteration"] = iteration
                    
                    # è®¾ç½®é»˜è®¤å€¼
                    if "priority" not in task:
                        task["priority"] = 5  # ä¸­ç­‰ä¼˜å…ˆçº§
                    if "estimated_impact" not in task:
                        task["estimated_impact"] = "ä¸­ç­‰å½±å“"
                
                # ç”Ÿæˆæ‰§è¡Œè®¡åˆ’
                result["execution_plan"] = generate_execution_plan(result["tasks"])
                
                # è®°å½•ä»»åŠ¡ç»Ÿè®¡
                task_stats = {
                    "ä»»åŠ¡æ•°é‡": len(result["tasks"]),
                    "ä»»åŠ¡ç±»å‹åˆ†å¸ƒ": ", ".join([f"{k}:{v}" for k, v in task_types.items()]),
                    "å¹³å‡ä¼˜å…ˆçº§": f"{sum(t.get('priority', 5) for t in result['tasks']) / len(result['tasks']):.1f}",
                    "æ‰§è¡Œè®¡åˆ’": "å·²ç”Ÿæˆ" if result.get("execution_plan") else "æœªç”Ÿæˆ"
                }
                log_detailed(self.paper_logger, "ğŸ“Š ä»»åŠ¡ç”Ÿæˆç»Ÿè®¡", task_stats)
                
                log_operation_success(self.paper_logger, "ä»»åŠ¡éªŒè¯å’Œå¢å¼º")
            else:
                self.paper_logger.warning("âš ï¸  æœªç”Ÿæˆä»»ä½•ä»»åŠ¡")
                result["tasks"] = []
            
            # æ·»åŠ å±‚çº§åˆ†æä¿¡æ¯
            result["level_analysis"] = {
                "level_code": level_code,
                "level_name": level_name,
                "focus_areas": focus_areas,
                "analysis_timestamp": get_current_timestamp(),
                "iteration": iteration,
                "tasks_generated": len(result.get("tasks", []))
            }
            
            # è®°å½•åˆ†æå®Œæˆ
            log_operation_success(self.paper_logger, f"{level_code}å±‚çº§åˆ†æ")
            self.paper_logger.info(f"âœ… {level_code}å±‚çº§åˆ†æå®Œæˆ: ç”Ÿæˆäº† {len(result.get('tasks', []))} ä¸ªä»»åŠ¡")
            
            # è®°å½•åˆ†æç»“æœæ‘˜è¦
            analysis_summary = {
                "å±‚çº§": f"{level_code} - {level_name}",
                "ç”Ÿæˆä»»åŠ¡æ•°": len(result.get("tasks", [])),
                "åˆ†æçŠ¶æ€": "æˆåŠŸ",
                "è€—æ—¶": "å·²å®Œæˆ"
            }
            if result.get("level_summary"):
                analysis_summary["AIæ€»ç»“"] = result["level_summary"][:100] + "..." if len(result.get("level_summary", "")) > 100 else result.get("level_summary", "")
            
            log_detailed(self.paper_logger, "ğŸ“‹ åˆ†æç»“æœæ‘˜è¦", analysis_summary)
            
            return result
            
        except Exception as e:
            error_msg = str(e)
            log_operation_error(self.paper_logger, f"{level_code}å±‚çº§åˆ†æ", error_msg)
            self.paper_logger.error(f"âŒ {level_code}å±‚çº§åˆ†æå‡ºç°å¼‚å¸¸: {error_msg}")
            
            # è®°å½•å¼‚å¸¸è¯¦æƒ…
            error_details = {
                "å¼‚å¸¸ç±»å‹": type(e).__name__,
                "å¼‚å¸¸æ¶ˆæ¯": error_msg,
                "å±‚çº§ä»£ç ": level_code,
                "è¿­ä»£æ¬¡æ•°": iteration,
                "å¤„ç†æ–¹å¼": "è¿”å›åå¤‡ç»“æœ"
            }
            log_detailed(self.paper_logger, "ğŸš¨ å¼‚å¸¸è¯¦æƒ…", error_details)
            
            return self._generate_fallback_result(level_code, f"åˆ†æå¼‚å¸¸: {error_msg}")
    
    def _build_level_system_prompt(self, level_code: str, level_name: str, 
                                 focus_areas: List[str], max_tasks: int) -> str:
        """æ„å»ºå±‚çº§ä¸“é—¨çš„ç³»ç»Ÿæç¤ºè¯"""
        
        focus_list = "ã€".join(focus_areas) if focus_areas else "ä»£ç æ”¹è¿›"
        
        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®ºæ–‡å¤ç°åˆ†æä¸“å®¶ã€‚ä½ æ­£åœ¨åˆ†æ{level_code}å±‚çº§ï¼ˆ{level_name}ï¼‰çš„ä»£ç æ”¹è¿›éœ€æ±‚ã€‚

                    ä¸“æ³¨é¢†åŸŸ: {focus_list}

                    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š
                    {{
                        "tasks": [
                            {{
                                "task_id": "ä»»åŠ¡å”¯ä¸€æ ‡è¯†",
                                "priority": ä¼˜å…ˆçº§æ•°å­—(1-10, 10æœ€é«˜),
                                "fixing_type": "add_file æˆ– change_file",
                                "which_file_to_fix": "éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶è·¯å¾„",
                                "fixing_plan_in_detail": "è¯¦ç»†çš„ä¿®å¤è®¡åˆ’ï¼Œè¯´æ˜å…·ä½“è¦åšä»€ä¹ˆ",
                                "raw_code": "éœ€è¦ä¿®æ”¹çš„åŸå§‹ä»£ç ç‰‡æ®µï¼ˆå¦‚æœæ˜¯ä¿®æ”¹æ–‡ä»¶ï¼‰",
                                "dependencies": ["ä¾èµ–çš„å…¶ä»–ä»»åŠ¡ID"],
                                "estimated_impact": "é¢„ä¼°å½±å“: é«˜å½±å“/ä¸­ç­‰å½±å“/ä½å½±å“"
                            }}
                        ],
                        "level_summary": "æœ¬å±‚çº§çš„æ•´ä½“åˆ†ææ€»ç»“",
                        "improvement_strategy": "é’ˆå¯¹{level_name}çš„æ”¹è¿›ç­–ç•¥"
                    }}

                    ä»»åŠ¡ç”Ÿæˆè¦æ±‚ï¼š
                    1. æœ€å¤šç”Ÿæˆ{max_tasks}ä¸ªæœ€é‡è¦çš„ä»»åŠ¡
                    2. ä¸“æ³¨äº{level_name}ç›¸å…³çš„æ”¹è¿›ï¼Œä¸è¦æ¶‰åŠå…¶ä»–å±‚çº§
                    3. ä»»åŠ¡è¦å…·ä½“å¯æ‰§è¡Œï¼Œæœ‰æ˜ç¡®çš„æ–‡ä»¶å’Œä¿®æ”¹è®¡åˆ’
                    4. ä¼˜å…ˆçº§è¦åˆç†ï¼Œå…³é”®ä»»åŠ¡ä¼˜å…ˆçº§æ›´é«˜
                    5. å¦‚æœå½“å‰å±‚çº§å·²ç»è¾ƒå¥½ï¼Œå¯ä»¥ç”Ÿæˆå°‘é‡ä¼˜åŒ–ä»»åŠ¡æˆ–è¿”å›ç©ºä»»åŠ¡åˆ—è¡¨"""
    #ç”¨try-exceptå¤„ç†
    def _build_level_user_prompt(self, level_code: str, level_description: str, 
                               paper_guide: str, indexed_repo_data: Dict[str, Any],
                               additional_content: str, target_metrics: Dict[str, Any], iteration: int) -> str:
        """æ„å»ºå±‚çº§ä¸“é—¨çš„ç”¨æˆ·æç¤ºè¯"""
        try:
            # ç›®æ ‡æŒ‡æ ‡éƒ¨åˆ†
            metrics_section = ""
            if target_metrics:
                metrics_section = f"""ç›®æ ‡æŒ‡æ ‡
                                    ```json
                                    {self._safe_json_dumps(target_metrics)}
                                    ```
                                    """
            
            # è¡¥å……ä¿¡æ¯éƒ¨åˆ†
            additional_section = ""
            if additional_content and additional_content.strip():
                additional_section = f"""
                ## è¡¥å……ä¿¡æ¯
                ```
                {additional_content}
                ```
                """
            
            # æ„å»ºæç¤ºè¯
            prompt = f"""è¿™æ˜¯ç¬¬{iteration}æ¬¡è¿­ä»£ï¼Œä¸“æ³¨äº{level_code}å±‚çº§çš„æ”¹è¿›ã€‚

                ## å½“å‰å±‚çº§ä¿¡æ¯
                - å±‚çº§: {level_code}
                - åç§°: {level_description}
                - è¿­ä»£: ç¬¬{iteration}æ¬¡

                ## è®ºæ–‡å¤ç°æŒ‡å—
                ```
                {paper_guide}
                ```{additional_section}
                {metrics_section}
                ## ä»£ç åº“ç´¢å¼•ä¿¡æ¯
                ```json
                {self._get_safe_repo_summary(indexed_repo_data)}
                ```

                ## åˆ†æè¦æ±‚

                è¯·ä¸“æ³¨äº{level_code}å±‚çº§ï¼Œåˆ†æå½“å‰ä»£ç åº“åœ¨ä»¥ä¸‹æ–¹é¢çš„æ”¹è¿›éœ€æ±‚ï¼š

                """
            # æ ¹æ®ä¸åŒå±‚çº§æ·»åŠ å…·ä½“è¦æ±‚
            if level_code == "L0":
                prompt += """L0ç¯å¢ƒæ­å»º - é‡ç‚¹åˆ†æï¼š
                            1. **ä¾èµ–ç®¡ç†**: requirements.txtæ˜¯å¦å®Œæ•´ï¼Ÿç¼ºå°‘å“ªäº›åŒ…ï¼Ÿ
                            2. **ç¯å¢ƒé…ç½®**: æ˜¯å¦æœ‰é…ç½®æ–‡ä»¶ï¼Ÿç¯å¢ƒå˜é‡è®¾ç½®ï¼Ÿ
                            3. **æ•°æ®å‡†å¤‡**: æ•°æ®åŠ è½½ä»£ç æ˜¯å¦æ­£ç¡®ï¼Ÿè·¯å¾„æ˜¯å¦é…ç½®ï¼Ÿ
                            4. **åŸºç¡€è¿è¡Œ**: mainæ–‡ä»¶èƒ½å¦æ­£å¸¸å¯¼å…¥å’Œæ‰§è¡Œï¼Ÿ
                            5. **é”™è¯¯ä¿®å¤**: ä¿®å¤é˜»æ­¢ç¨‹åºè¿è¡Œçš„åŸºç¡€é”™è¯¯

                            ç”Ÿæˆçš„ä»»åŠ¡åº”è¯¥è®©ç¨‹åºèƒ½å¤Ÿ**æˆåŠŸè¿è¡Œèµ·æ¥**ã€‚"""
                
            elif level_code == "L1":
                prompt += """L1æ ¸å¿ƒç®—æ³• - é‡ç‚¹åˆ†æï¼š
                            1. **ç®—æ³•å®ç°**: æ ¸å¿ƒç®—æ³•æ˜¯å¦æŒ‰è®ºæ–‡æè¿°å®ç°ï¼Ÿ
                            2. **æ¨¡å‹æ¶æ„**: ç¥ç»ç½‘ç»œç»“æ„æ˜¯å¦æ­£ç¡®ï¼Ÿ
                            3. **å…³é”®å…¬å¼**: é‡è¦çš„æ•°å­¦å…¬å¼æ˜¯å¦æ­£ç¡®å®ç°ï¼Ÿ
                            4. **æ ¸å¿ƒç»„ä»¶**: å…³é”®çš„ç±»å’Œå‡½æ•°æ˜¯å¦å­˜åœ¨ä¸”å®Œæ•´ï¼Ÿ
                            5. **ç®—æ³•é€»è¾‘**: ç®—æ³•æµç¨‹æ˜¯å¦ç¬¦åˆè®ºæ–‡æè¿°ï¼Ÿ

                            ç”Ÿæˆçš„ä»»åŠ¡åº”è¯¥è®©**æ ¸å¿ƒç®—æ³•å®ç°æ­£ç¡®**ã€‚"""
                
            elif level_code == "L2":
                prompt += """L2è®­ç»ƒæµç¨‹ - é‡ç‚¹åˆ†æï¼š
                            1. **è®­ç»ƒå¾ªç¯**: è®­ç»ƒè¿‡ç¨‹æ˜¯å¦å®Œæ•´ï¼Ÿ
                            2. **æŸå¤±è®¡ç®—**: æŸå¤±å‡½æ•°æ˜¯å¦æ­£ç¡®å®ç°ï¼Ÿ
                            3. **ä¼˜åŒ–å™¨**: ä¼˜åŒ–å™¨é…ç½®æ˜¯å¦åˆé€‚ï¼Ÿ
                            4. **æ•°æ®æµæ°´çº¿**: æ•°æ®åŠ è½½å’Œé¢„å¤„ç†æ˜¯å¦æ­£ç¡®ï¼Ÿ
                            5. **æ¨ç†æµç¨‹**: æ¨ç†è¿‡ç¨‹æ˜¯å¦å®Œæ•´ï¼Ÿ

                            ç”Ÿæˆçš„ä»»åŠ¡åº”è¯¥è®©**æ•´ä¸ªè®­ç»ƒæ¨ç†æµç¨‹èƒ½å¤Ÿé¡ºåˆ©è¿è¡Œ**ã€‚"""
                
            elif level_code == "L3":
                prompt += """L3å®éªŒå¤ç° - é‡ç‚¹åˆ†æï¼š
                            1. **å®éªŒè„šæœ¬**: æ˜¯å¦æœ‰å®Œæ•´çš„å®éªŒè¿è¡Œè„šæœ¬ï¼Ÿ
                            2. **è¯„ä¼°æŒ‡æ ‡**: è¯„ä¼°ä»£ç æ˜¯å¦å®Œæ•´ï¼ŸæŒ‡æ ‡è®¡ç®—æ˜¯å¦æ­£ç¡®ï¼Ÿ
                            3. **å¤šä¸ªå®éªŒ**: è®ºæ–‡ä¸­çš„å„ä¸ªå®éªŒæ˜¯å¦éƒ½èƒ½è¿è¡Œï¼Ÿ
                            4. **ç»“æœè¾“å‡º**: å®éªŒç»“æœæ˜¯å¦æ­£ç¡®ä¿å­˜å’Œå±•ç¤ºï¼Ÿ
                            5. **æ•°æ®é›†æ”¯æŒ**: æ˜¯å¦æ”¯æŒè®ºæ–‡ä¸­ä½¿ç”¨çš„æ‰€æœ‰æ•°æ®é›†ï¼Ÿ

                            ç”Ÿæˆçš„ä»»åŠ¡åº”è¯¥è®©**æ‰€æœ‰è®ºæ–‡å®éªŒéƒ½èƒ½æ­£å¸¸è¿è¡Œ**ã€‚"""
                
            elif level_code == "L4":
                prompt += """L4ç»“æœå¯¹é½ - é‡ç‚¹åˆ†æï¼š
                            1. **ç»“æœæ¯”è¾ƒ**: å½“å‰ç»“æœä¸è®ºæ–‡ç»“æœçš„å·®è·ï¼Ÿ
                            2. **å‚æ•°è°ƒä¼˜**: è¶…å‚æ•°æ˜¯å¦éœ€è¦è°ƒæ•´ï¼Ÿ
                            3. **æ€§èƒ½ä¼˜åŒ–**: æ˜¯å¦æœ‰æ€§èƒ½ç“¶é¢ˆéœ€è¦ä¼˜åŒ–ï¼Ÿ
                            4. **æ•°å€¼ç²¾åº¦**: æ•°å€¼è®¡ç®—ç²¾åº¦æ˜¯å¦è¶³å¤Ÿï¼Ÿ
                            5. **å®éªŒè®¾ç½®**: å®éªŒè®¾ç½®æ˜¯å¦ä¸è®ºæ–‡å®Œå…¨ä¸€è‡´ï¼Ÿ

                            ç”Ÿæˆçš„ä»»åŠ¡åº”è¯¥è®©**å®éªŒç»“æœå°½å¯èƒ½æ¥è¿‘è®ºæ–‡æŠ¥å‘Šçš„æ•°å€¼**ã€‚"""
            
            prompt += """è¯·åŸºäºä¸Šè¿°åˆ†æè¦æ±‚ï¼Œç”Ÿæˆé’ˆå¯¹å½“å‰å±‚çº§çš„å…·ä½“æ”¹è¿›ä»»åŠ¡ã€‚ä¸€å®šè¦ç»“åˆå¤ç°æŒ‡å—å¯¹æ¯ä¸€ä¸ªtaskç»™å‡ºè¯¦ç»†çš„æ‰§è¡Œæ–¹æ¡ˆ"""

            return prompt
        except Exception as e:
            self.paper_logger.error(f"âŒ æ„å»ºç”¨æˆ·æç¤ºè¯å¤±è´¥: {str(e)}")
            raise e
    
    def _generate_fallback_result(self, level_code: str, reason: str) -> Dict[str, Any]:
        """ç”Ÿæˆå±‚çº§åˆ†æçš„åå¤‡ç»“æœ"""
        return {
            "tasks": [],
            "level_analysis": {
                "level_code": level_code,
                "level_name": self.level_strategies.get(level_code, {}).get("name", level_code),
                "analysis_timestamp": get_current_timestamp(),
                "tasks_generated": 0,
                "fallback_reason": reason
            },
            "level_summary": f"{level_code}å±‚çº§åˆ†æå¤±è´¥: {reason}",
            "improvement_strategy": "å»ºè®®æ‰‹åŠ¨æ£€æŸ¥ä»£ç åº“çŠ¶æ€",
            "execution_plan": {
                "total_tasks": 0,
                "execution_order": [],
                "risk_assessment": "åˆ†æå¤±è´¥ï¼Œæ— æ³•è¯„ä¼°é£é™©"
            }
        }
    
    def _get_safe_repo_summary(self, indexed_repo_data: Dict[str, Any]) -> str:
        """
        å®‰å…¨åœ°è·å–ä»£ç åº“æ‘˜è¦ä¿¡æ¯ï¼Œé¿å…å¤æ‚åµŒå¥—å­—å…¸åºåˆ—åŒ–é—®é¢˜
        
        å‚æ•°:
            indexed_repo_data (Dict[str, Any]): åŸå§‹çš„ä»£ç åº“ç´¢å¼•æ•°æ®
        
        è¿”å›:
            str: å®‰å…¨çš„JSONå­—ç¬¦ä¸²
        """
        try:
            # å®‰å…¨åœ°å¤„ç†fileså­—æ®µï¼Œç®€åŒ–å¤æ‚çš„åµŒå¥—ç»“æ„
            files_summary = {}
            original_files = indexed_repo_data.get("files", {})
            
            # åªå–å‰10ä¸ªæ–‡ä»¶ï¼Œå¹¶ç®€åŒ–æ¯ä¸ªæ–‡ä»¶çš„ä¿¡æ¯
            file_count = 0
            for file_path, file_data in original_files.items():
                if file_count >= 10:
                    break
                
                # ç®€åŒ–æ–‡ä»¶ä¿¡æ¯ï¼Œåªä¿ç•™åŸºæœ¬ç»“æ„
                simplified_file_info = {
                    "functions": len(file_data.get("functions", [])),
                    "classes": list(file_data.get("classes", {}).keys())[:5],  # åªä¿ç•™å‰5ä¸ªç±»å
                    "has_functions": len(file_data.get("functions", [])) > 0,
                    "has_classes": len(file_data.get("classes", {})) > 0
                }
                
                files_summary[file_path] = simplified_file_info
                file_count += 1
            
            # æ„å»ºå®‰å…¨çš„æ•°æ®ç»“æ„
            safe_data = {
                "project_name": indexed_repo_data.get("project_name", ""),
                "directory_structure": indexed_repo_data.get("directory_structure", [])[:15],
                "files_summary": files_summary,
                "total_files": len(original_files),
                "analysis_info": {
                    "has_dependency_analysis": indexed_repo_data.get("analysis_info", {}).get("has_dependency_analysis", False),
                    "analyzer_timestamp": indexed_repo_data.get("analysis_info", {}).get("analyzer_timestamp", "")
                }
            }
            
            return self._safe_json_dumps(safe_data)
            
        except Exception as e:
            # å¦‚æœé¢„å¤„ç†å¤±è´¥ï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
            return self._safe_json_dumps({
                "error": f"ä»£ç åº“ä¿¡æ¯å¤„ç†å¤±è´¥: {str(e)}",
                "project_name": indexed_repo_data.get("project_name", ""),
                "total_files": len(indexed_repo_data.get("files", {})),
                "has_directory_structure": len(indexed_repo_data.get("directory_structure", [])) > 0
            })
    
    def _safe_json_dumps(self, data: Any) -> str:
        """å®‰å…¨çš„JSONåºåˆ—åŒ–ï¼Œé¿å…unhashable typeé”™è¯¯"""
        try:
            return json.dumps(data, indent=2, ensure_ascii=False)
        except (TypeError, ValueError) as e:
            # å¦‚æœåºåˆ—åŒ–å¤±è´¥ï¼Œè¿”å›å­—ç¬¦ä¸²è¡¨ç¤º
            return f'{{"error": "JSONåºåˆ—åŒ–å¤±è´¥: {str(e)}", "data_type": "{type(data).__name__}"}}' 

if __name__ == "__main__":
    # å¯¼å…¥å¿…è¦çš„å·¥å…·å‡½æ•°
    from .utils import load_additional_guides
    
    analyzer = PaperAnalyzerAgent()
    
    # ä½¿ç”¨è¡¥å……ä¿¡æ¯æ–‡æ¡£è·¯å¾„
    additional_guides_paths = [
        "/Users/wwchdemac/python_projects/debug_agent/test_papers/paper_test_1_addendum.md"
    ]
    
    # åŠ è½½è¡¥å……ä¿¡æ¯å†…å®¹
    additional_result = load_additional_guides(additional_guides_paths)
    additional_content = additional_result["additional_content"] if additional_result["success"] else ""
    
    result = analyzer.analyze_paper_level(
        level_code="L1",
        level_description="ç¯å¢ƒæ­å»º",
        #ç”¨test paper 1 çš„guide
        paper_guide="/Users/wwchdemac/python_projects/debug_agent/test_papers/paper_test_1_reproduction_guide.md",
        #ç”¨RICEçš„repoåœ°å€
        repo_path="/Users/wwchdemac/python_projects/debug_agent/test_input/rice/submission",
        target_metrics={},
        additional_content=additional_content,  # ä½¿ç”¨å¤„ç†åçš„è¡¥å……ä¿¡æ¯å†…å®¹
        iteration=1
    )
    print('==================result====================')
    print(json.dumps(result, indent=4, ensure_ascii=False))
    print('==================result====================')
    print('\n\n')